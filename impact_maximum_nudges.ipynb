{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the maximum impacts for individual, local, synergistic and global nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import probability_distributions\n",
    "import maximum_nudges\n",
    "import evolutionary_algorithms as ea\n",
    "import maximum_nudges_evolutionary as ev_max_nudges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook level constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUDGE_SIZE = 0.01\n",
    "NUMBER_OF_STATES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First generate a generic input distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution parameters\n",
    "input_variables = 2\n",
    "number_of_states = 5\n",
    "\n",
    "#generate both input and conditional output with Dirichlet weights\n",
    "distribution_shape = [number_of_states]*input_variables\n",
    "total_number_of_states = reduce(lambda x,y: x*y, distribution_shape)\n",
    "input_dist = np.random.dirichlet([1]*total_number_of_states)\n",
    "input_dist = np.reshape(input_dist, distribution_shape)\n",
    "cond_shape = [number_of_states]*(input_variables+1)\n",
    "cond_output = [\n",
    "    probability_distributions.compute_joint_uniform_random((number_of_states,))\n",
    "    for i in range(number_of_states**(input_variables))\n",
    "]\n",
    "cond_output = np.array(cond_output)\n",
    "cond_output = np.reshape(cond_output, cond_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the generated input and conditional output distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/joboti/azumi_derkjan/master_thesis/code/\"\n",
    "INPUT_FOLDER = \"input_distributions/\"\n",
    "COND_OUTPUT_FOLDER = \"conditional_output_distributions/\"\n",
    "FOLDER_FORMAT_INPUT = \"{}var_{}states/\"\n",
    "FOLDER_FORMAT_CONDITIONAL = \"{}var_{}states/\"\n",
    "FILE_FORMAT_INPUT = \"dist_{}.npy\"\n",
    "FILE_FORMAT_COND_OUTPUT = \"cond_dist_{}.npy\"\n",
    "\n",
    "DIRICHLET_FOLDER_INPUT = \"dirichlet/\"\n",
    "ENTROPY_LOW_FOLDER_INPUT = \"entropy_0.5/\"\n",
    "ENTROPY_MEDIUM_FOLDER_INPUT = \"entropy_0.75/\"\n",
    "\n",
    "DIRICHLET_FOLDER_COND_OUTPUT = \"dirichlet/\"\n",
    "\n",
    "\n",
    "def generate_distributions(path_to_files, file_format, number_of_distributions):\n",
    "    for i in range(number_of_distributions):\n",
    "        file_name = path_to_files + file_format.format(i)\n",
    "        with open(file_name, 'rb') as f:\n",
    "            yield np.load(f)\n",
    "            \n",
    "def generate_input_and_conditional_output(input_type, parameters, cond_output_type=\"dirichlet\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_type: string in set {\"dirichlet\", \"entropy_0.5\", \"entropy_0.75\"}\n",
    "    parameters: dict\n",
    "    cond_output: \n",
    "    \n",
    "    Returns: a dict with keys:\n",
    "    -------\n",
    "    number_of_var: a number\n",
    "    number_of_states: a number\n",
    "    input_dist: nd-array\n",
    "    cond_output: nd-array\n",
    "    \n",
    "    \"\"\"\n",
    "    if input_type == \"dirichlet\":\n",
    "        input_dirichlet_path = PATH + INPUT_FOLDER + DIRICHLET_FOLDER_INPUT\n",
    "    elif input_type == \"entropy_0.75\":\n",
    "        input_dirichlet_path = PATH + INPUT_FOLDER + ENTROPY_MEDIUM_FOLDER_INPUT\n",
    "    elif input_type == \"entropy_0.5\":\n",
    "        input_dirichlet_path = PATH + INPUT_FOLDER + ENTROPY_LOW_FOLDER_INPUT\n",
    "    else:\n",
    "        raise ValueError(\"supply valid input distribution type\")\n",
    "\n",
    "    cond_output_dirichlet_path = PATH + COND_OUTPUT_FOLDER + DIRICHLET_FOLDER_COND_OUTPUT\n",
    "\n",
    "    min_inputs = parameters[\"min_number_inputs\"]\n",
    "    max_inputs = parameters[\"max_number_inputs\"]\n",
    "    number_of_states = parameters[\"number_of_states\"]\n",
    "    for number_of_var in range(min_inputs, max_inputs, 1):\n",
    "        path_to_input_files = (\n",
    "            input_dirichlet_path \n",
    "            + FOLDER_FORMAT_INPUT.format(number_of_var, number_of_states)\n",
    "        )\n",
    "        path_to_cond_output_files = (\n",
    "            cond_output_dirichlet_path \n",
    "            + FOLDER_FORMAT_CONDITIONAL.format(number_of_var, number_of_states)\n",
    "        )\n",
    "        input_generator = generate_distributions(\n",
    "            path_to_input_files, FILE_FORMAT_INPUT, \n",
    "            parameters[\"number_of_distributions\"]\n",
    "        )\n",
    "        cond_output_generator = generate_distributions(\n",
    "            path_to_cond_output_files, FILE_FORMAT_COND_OUTPUT, \n",
    "            parameters[\"number_of_distributions\"]\n",
    "        )\n",
    "        input_shape = [number_of_states]*number_of_var\n",
    "        cond_output_shape = [number_of_states]*(number_of_var+1)\n",
    "        for sample in range(parameters[\"number_of_distributions\"]):\n",
    "            input_dist = next(input_generator)\n",
    "            input_dist = np.reshape(input_dist, input_shape)\n",
    "            cond_output = next(cond_output_generator)\n",
    "            cond_output = np.reshape(cond_output, cond_output_shape)\n",
    "            yield {\n",
    "                \"number_of_vars\": number_of_var,\n",
    "                \"number_of_states\": parameters[\"number_of_states\"],\n",
    "                \"input_dist\": input_dist,\n",
    "                \"cond_output\": cond_output\n",
    "            }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_distributions = {\n",
    "    \"max_number_inputs\": 6,\n",
    "    \"min_number_inputs\": 1,\n",
    "    \"number_of_states\": 5,\n",
    "    \"number_of_distributions\": 100\n",
    "}\n",
    "\n",
    "generator = generate_input_and_conditional_output(\n",
    "    'dirichlet', parameters_distributions, cond_output_type=\"dirichlet\"\n",
    ")\n",
    "\n",
    "for dist_dict in generator:\n",
    "    a = dist_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the function to find max impact local nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimum_local_nudge(input_dist, cond_output, number_of_input_variables, \n",
    "                             number_of_states, nudge_size, parameters):\n",
    "    \"\"\"optimize local nudge\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_dist:nd-array\n",
    "    cond_output: nd-array, one axis more than input_dist\n",
    "    \n",
    "    \"\"\"\n",
    "    local_nudges = []\n",
    "    for _ in range(parameters[\"population_size\"]):\n",
    "        new_local_nudge = ev_max_nudges.LocalNudge.create_local_nudge(\n",
    "            parameters[\"nudged_vars_to_states\"], nudge_size, \n",
    "            parameters[\"mutation_size_weights\"], parameters[\"start_mutation_size\"],\n",
    "            parameters[\"change_mutation_size\"], timestamp=0\n",
    "        )\n",
    "        local_nudges.append(new_local_nudge)\n",
    "\n",
    "    for local_nudge in local_nudges:\n",
    "        local_nudge.evaluate(input_dist, cond_output)\n",
    "\n",
    "    initial_impact = ea.sort_individuals(local_nudges)[0].score\n",
    "\n",
    "    #start the optimization process\n",
    "    find_max_local_nudge = ev_max_nudges.FindMaximumLocalNudge(\n",
    "        input_dist, cond_output, nudge_size, \n",
    "        parameters[\"generational\"], parameters[\"number_of_children\"], \n",
    "        parameters[\"parent_selection_mode\"]\n",
    "    )\n",
    "    max_local_nudge_individual = find_max_local_nudge.get_max_nudge(\n",
    "        local_nudges, parameters[\"number_of_generations\"]\n",
    "    )\n",
    "    max_impact = max_local_nudge_individual.score \n",
    "    print(\"local nudge: initial impact {} max impact {}\".format(initial_impact, max_impact))\n",
    "    return max_impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_max_nudges.TEST = False\n",
    "\n",
    "parameters = {\n",
    "    \"number_of_generations\": 250, \n",
    "    \"population_size\": 10,\n",
    "    \"number_of_children\": 20, \n",
    "    \"generational\": True,\n",
    "    \"mutation_size\": NUDGE_SIZE/4,\n",
    "    \"parent_selection_mode\": \"rank_exponential\",\n",
    "    \"mutation_size_weights\": 0.025,\n",
    "    \"start_mutation_size\": NUDGE_SIZE/7.5,\n",
    "}\n",
    "parameters[\"change_mutation_size\"] = parameters[\"start_mutation_size\"]/12\n",
    "parameters[\"nudged_vars_to_states\"] = None\n",
    "\n",
    "parameters_distributions = {\n",
    "    \"max_number_inputs\": 6,\n",
    "    \"min_number_inputs\": 2,\n",
    "    \"number_of_states\": 5,\n",
    "    \"number_of_distributions\": 100\n",
    "}\n",
    "\n",
    "generator = generate_input_and_conditional_output(\n",
    "    'dirichlet', parameters_distributions, cond_output_type=\"dirichlet\"\n",
    ")\n",
    "\n",
    "#change file name before turning run on\n",
    "RUN = False\n",
    "FILE_NAME = \"max_impact_local_nudges_dirichlet2.json\"\n",
    "if RUN:\n",
    "    max_local_impact_dict = {}\n",
    "    prev_number_of_vars = -1\n",
    "    for count, dist_dict in enumerate(generator):\n",
    "        number_of_vars = dist_dict[\"number_of_vars\"]\n",
    "        number_of_states = dist_dict[\"number_of_states\"]\n",
    "        if number_of_vars != len(dist_dict[\"input_dist\"].shape):\n",
    "            print(\"WARNING in sample {} input dist has weird distribution\".format(count))\n",
    "        if number_of_vars != prev_number_of_vars:\n",
    "            prev_number_of_vars = number_of_vars \n",
    "            print(\"the number of vars {}\".format(number_of_vars))\n",
    "        #print(dist_dict[\"cond_output\"].shape)\n",
    "        parameters[\"nudged_vars_to_states\"] = {\n",
    "            nudged_var:number_of_states for nudged_var in range(number_of_vars)\n",
    "        }\n",
    "        max_impact = find_optimum_local_nudge(\n",
    "            dist_dict[\"input_dist\"], dist_dict[\"cond_output\"], number_of_vars,\n",
    "            number_of_states, NUDGE_SIZE, parameters=parameters\n",
    "        )\n",
    "        if number_of_vars in max_local_impact_dict:\n",
    "            max_local_impact_dict[number_of_vars].append(max_impact)\n",
    "        else:\n",
    "            max_local_impact_dict[number_of_vars] = [max_impact]\n",
    "\n",
    "        if (count+1)%5==0 and count != 0:\n",
    "            with open(FILE_NAME, 'w') as f:\n",
    "                json.dump(max_local_impact_dict, f, indent=4)\n",
    "                \n",
    "    with open(FILE_NAME, 'w') as f:\n",
    "        json.dump(max_local_impact_dict, f, indent=4)\n",
    "            \n",
    "    print(max_local_impact_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the maximum individual impact evolutionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the maximum impact of an individual nudge exactly\n",
    "\n",
    "Due to different definitions of nudge size used, the nudge size needs to be divided by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the actual maximum individual nudge 0.00149752215863\n"
     ]
    }
   ],
   "source": [
    "def find_max_impact_individual_nudge_exactly(input_dist, cond_output, nudge_size, \n",
    "                                             without_conditional=False):\n",
    "    new_input_dist = np.copy(input_dist)\n",
    "    max_impacts = []\n",
    "    if not without_conditional:\n",
    "        for i in range(len(new_input_dist.shape)):\n",
    "            new_input_dist = np.swapaxes(new_input_dist, i,\n",
    "                                         len(new_input_dist.shape)-1)\n",
    "            max_impact = maximum_nudges.find_maximum_local_nudge(\n",
    "                new_input_dist, cond_output, nudge_size/2\n",
    "            )\n",
    "            max_impacts.append(max_impact)\n",
    "            new_input_dist = np.swapaxes(new_input_dist, i,\n",
    "                                         len(new_input_dist.shape)-1)\n",
    "\n",
    "            return max(max_impacts)\n",
    "    else:\n",
    "        for i in range(len(new_input_dist.shape)):\n",
    "            new_input_dist = np.swapaxes(new_input_dist, i,\n",
    "                                         len(new_input_dist.shape)-1)\n",
    "            max_impact = maximum_nudges.find_maximum_local_nudge_without_conditional(\n",
    "                new_input_dist, cond_output, nudge_size/2\n",
    "            )\n",
    "            max_impacts.append(max_impact)\n",
    "            new_input_dist = np.swapaxes(new_input_dist, i,\n",
    "                                         len(new_input_dist.shape)-1)\n",
    "\n",
    "            return max(max_impacts)\n",
    "            \n",
    "max_impact = find_max_impact_individual_nudge_exactly(input_dist, cond_output, NUDGE_SIZE/2, True)\n",
    "print(\"the actual maximum individual nudge {}\".format(max_impact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Run the experiment for dirichlet inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_distributions = {\n",
    "    \"max_number_inputs\": 7,\n",
    "    \"min_number_inputs\": 1,\n",
    "    \"number_of_states\": 5,\n",
    "    \"number_of_distributions\": 100\n",
    "}\n",
    "\n",
    "generator = generate_input_and_conditional_output(\n",
    "    'dirichlet', parameters_distributions, cond_output_type=\"dirichlet\"\n",
    ")\n",
    "\n",
    "#set new file name before turning run on\n",
    "RUN = False\n",
    "FILE_NAME = \"max_impact_individual_nudges_dirichlet2_first100samples.json\"\n",
    "if RUN:\n",
    "    max_individual_impact_dict = {}\n",
    "    prev_number_of_vars = -1\n",
    "    for count, dist_dict in enumerate(generator):\n",
    "        print(count)\n",
    "        number_of_vars = dist_dict[\"number_of_vars\"]\n",
    "        number_of_states = dist_dict[\"number_of_states\"]\n",
    "        if number_of_vars != len(dist_dict[\"input_dist\"].shape):\n",
    "            print(\"WARNING in sample {} input dist has weird distribution\".format(count))\n",
    "        if number_of_vars != prev_number_of_vars:\n",
    "            prev_number_of_vars = number_of_vars \n",
    "            print(\"the number of vars {}\".format(number_of_vars))\n",
    "        #print(dist_dict[\"cond_output\"].shape)\n",
    "        max_impact = find_max_impact_individual_nudge_exactly(\n",
    "            dist_dict[\"input_dist\"], dist_dict[\"cond_output\"], NUDGE_SIZE/2\n",
    "        )\n",
    "        if number_of_vars in max_individual_impact_dict:\n",
    "            max_individual_impact_dict[number_of_vars].append(max_impact)\n",
    "        else:\n",
    "            max_individual_impact_dict[number_of_vars] = [max_impact]\n",
    "\n",
    "        if (count+1)%5==0 and count != 0:\n",
    "            with open(FILE_NAME, 'w') as f:\n",
    "                json.dump(max_individual_impact_dict, f, indent=4)\n",
    "\n",
    "    with open(FILE_NAME, 'w') as f:\n",
    "        json.dump(max_individual_impact_dict, f, indent=4)\n",
    "\n",
    "    print(max_individual_impact_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the experiment for  distributions with 75 percent of the maximum entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "the number of vars 1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "the number of vars 2\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "the number of vars 3\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "the number of vars 4\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "the number of vars 5\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "the number of vars 6\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "{1: [0.002869980638571158, 0.0021692854770400216, 0.0035808422126994187, 0.0035654225239459434, 0.0027033736880852314, 0.0033994825140872494, 0.0032391054510735218, 0.002757913774295114, 0.0034361881020422741, 0.0036156831527425892, 0.003765416499510899, 0.0028673101979885618, 0.0044499691569279652, 0.0034748688909535504, 0.0036615639735057062, 0.0036060728662983248, 0.0035358856959060618, 0.0031501643528031105, 0.0027616033644059686, 0.0036561580319108365, 0.0029378347971284521, 0.0037794790684181558, 0.0033979392510503424, 0.0029686812890558278, 0.0037022255160396693, 0.0039224014728161134, 0.0031290339164652204, 0.003558083150738705, 0.0040464932983326866, 0.0034007537062885297, 0.0027279737128037477, 0.0035418894539833391, 0.0035045615488586016, 0.0018339816865144988, 0.0035516301313285635, 0.0025002677468204592, 0.00398914475212172, 0.0031805433837213183, 0.0032834565407300568, 0.003247547684578377, 0.0033839757131908631, 0.0037658291288527749, 0.0029655523012631826, 0.0034810343744358825, 0.0037109603394240125, 0.0030608639599223049, 0.0036207305203070986, 0.0035173894463882451, 0.0029629024675259887, 0.0024760288367207148, 0.0033905931625245779, 0.003041514872349963, 0.0031786067788041874, 0.0028893720345036901, 0.0031988645448900615, 0.0036419991985507426, 0.0032617547458508424, 0.0023857613024473329, 0.0022398045757589268, 0.0029787770892292596, 0.0035734597067082702, 0.003073756022764741, 0.0037964291900597228, 0.0033962711916763174, 0.0036053332227777308, 0.0022176342775839688, 0.0022238581407687239, 0.0027117451373922085, 0.0032523798521457239, 0.0038249843808986429, 0.0028957345150839868, 0.0029857554047151356, 0.0033017566646299703, 0.004020242531714859, 0.0037464991194766347, 0.0040578865721571725, 0.0035406523493211363, 0.002826632467173257, 0.0035172844554785818, 0.003036647890074678, 0.0026504940904544336, 0.0028558745814653021, 0.0027250277826367693, 0.0028607372603578073, 0.0029089781225443419, 0.002333593191257415, 0.0029504286259276532, 0.0030123509297721586, 0.0042594656150929224, 0.0029527350013892639, 0.0036346540769480011, 0.0032895753769626957, 0.0030953597223029841, 0.0034897656811488679, 0.0047637907487041269, 0.0032041521008868796, 0.0036072203438088286, 0.0025551158038498948, 0.0028919000981496739, 0.0029527112840737431, 0.0040502629488016993, 0.0043625760874935245, 0.0037942216338700714, 0.0030472366789931708, 0.0027780061719160486, 0.0031981108353817712, 0.0040674907060128645, 0.0031138599175184393, 0.0032422047938585712, 0.0037272693795524833, 0.0027494953155794929, 0.0042540005984589967, 0.0027629403169142728, 0.0037016627937290327, 0.0039008743322106491, 0.0032777371908914361, 0.003378669560905614, 0.0032484956117743236, 0.0027183946284466982, 0.0033900022454154584, 0.0032371288513476457, 0.0031516843263262456, 0.0032498517478382364, 0.0027050811849791906, 0.0038301902179534459, 0.00406511613491959, 0.0036861797020202205, 0.0037027730101432598, 0.0037944377678079001, 0.0036823575634396125, 0.0035039633553324012, 0.0038969503129435796, 0.00330399489544775, 0.0033399939017442325, 0.0027909825848079308, 0.0036026158143334527, 0.0033839767278173159, 0.0025363197088394602, 0.0029655465744300902, 0.0031353575787827449, 0.0029352923570606475, 0.0038439692751821852, 0.0026616919659881127, 0.0025627692039328666, 0.003463830998357063, 0.0024785784850308613, 0.0032030750154787083, 0.0033146886570504974, 0.0036146584765054889, 0.0034911641455301163, 0.0033699886953167286, 0.0040231496303602286, 0.0032921707111948414, 0.0020957185221801809, 0.003444233500461579, 0.0031473536279481565, 0.0026662754130102606, 0.0028891367107692712, 0.003260132989283516, 0.0021593309929219375, 0.003444025005838393, 0.003170738041774172, 0.0043741400383374065, 0.0028124788431492416, 0.0036105782085590994, 0.0017974371070724152, 0.0036921166112012195, 0.002617112070143186, 0.0044099612119581199, 0.0039270338228619755, 0.0038577877269670088, 0.0028452579986491498, 0.0033936701329841546, 0.0031244964414835772, 0.0035331632216152632, 0.0035942342023552974, 0.0041401054005744033, 0.0028774354663455479, 0.0026397694759071385, 0.0033385221025149263, 0.0025155346971832675, 0.0034848776704603144, 0.0039440933073110022, 0.0039584133740115275, 0.0029041074060122459, 0.0033716234913513223, 0.0032071354568399928, 0.0023549231880294627, 0.0032586912006106563, 0.0035341504565727178, 0.0032722206457861149, 0.0027399271850041939, 0.0030873369372759881, 0.0024484224086113882, 0.0029185836097675721, 0.003381793807953358, 0.0033980697274462023, 0.0038500516913389598, 0.0036501306623599366, 0.0029504403401319373], 2: [0.0017713941705380211, 0.0016985317343355543, 0.002011824790034878, 0.0019198104585684197, 0.0017239796319267347, 0.0016252949918100356, 0.0018106060881180391, 0.0027451909131986103, 0.0023690134732581442, 0.0021764903514606779, 0.0019983738050312239, 0.0019051267308315624, 0.0015272759169224224, 0.0015881661618027641, 0.0018358364568263476, 0.0020211623689616537, 0.0018209382170263, 0.0021783806455619135, 0.0027003865919459184, 0.0022049742803464858, 0.0019737315355768358, 0.0020086876486169526, 0.001826396079697433, 0.0021382167305586089, 0.0019468538765546773, 0.0017752577761612571, 0.0018076392483887668, 0.0017475733348498079, 0.0019422835796348678, 0.0017146049633296342, 0.001811571306413537, 0.00176359837591431, 0.002104560828237951, 0.0024764843084273758, 0.001473945767980483, 0.0020343588855561709, 0.0019013955299999458, 0.0018518977845194647, 0.0024524376666910526, 0.0020254581375521801, 0.0026044321736762321, 0.0017564068946735146, 0.0021023216980190188, 0.0020034982531189783, 0.0019374461443780911, 0.0016263785738420251, 0.001857228480572938, 0.0019710616090721539, 0.0016792042323708189, 0.002204354582595477, 0.0016464148091690852, 0.0016817741756358588, 0.0021257266802801925, 0.0017221926300604999, 0.0022125455635418801, 0.0025176057049410426, 0.0023786169999309263, 0.0018244011606045876, 0.0020649944575353968, 0.0021480591279707676, 0.0019876654709692623, 0.0017304781828934716, 0.0023262965074715715, 0.0020797501621778371, 0.0019924050158333937, 0.0019385398177994794, 0.0017445590535967931, 0.0016574300457338187, 0.0019694030381116959, 0.0027815202825917738, 0.0015940781786325314, 0.0019081547288828751, 0.0019892658526928595, 0.001209026747205689, 0.0021143609644372853, 0.0020446697880789951, 0.0020975008527391371, 0.0016824600584266622, 0.0022364461107652616, 0.001821224394085334, 0.0017583660191508312, 0.0021793446848034073, 0.0022278555242775883, 0.0021332952303099572, 0.0014922359172764841, 0.0020046346957216759, 0.0020392491788737361, 0.0016689071101849251, 0.0017263713218997018, 0.0019875254040705097, 0.0020728483846531383, 0.0018944835492261986, 0.0016634043922484857, 0.0017462213743767968, 0.002264752860829168, 0.0016615647478433111, 0.0019043749955658343, 0.0023350955599015701, 0.0019606316974715011, 0.0018414422643496272, 0.0024668773191082096, 0.0014481150030892914, 0.0019124585532184382, 0.0020737647337912349, 0.0017910584724426357, 0.002142473418602491, 0.0012602595448869291, 0.0022660373187179859, 0.0021660075250960961, 0.0018444906797602538, 0.0019709313315587579, 0.0020281143032794212, 0.0020982178785824094, 0.0012258692723796513, 0.0017725376693098617, 0.0020220987672956233, 0.0018688990793270422, 0.00206004856497004, 0.0015925119876111617, 0.0019381950857507233, 0.0023558395740920115, 0.0020038375575112213, 0.0019475480069888224, 0.0017986619021977809, 0.001731175431318817, 0.0024446712768768052, 0.0014176782101232115, 0.0024325648223009924, 0.0019752222905535222, 0.0017626223346030308, 0.0021544357758094225, 0.0018557358202725158, 0.0021510390616896829, 0.0021031416539040145, 0.0016488440573957138, 0.0020532437234200853, 0.0019886384901965548, 0.0020572893686446698, 0.0021950301887197878, 0.0019651020003397502, 0.0015847319733918739, 0.0032552758012880955, 0.0023372021110107553, 0.0018336127457794071, 0.0025337088973072667, 0.0015232069980980322, 0.0021820618183354262, 0.0018462310615698688, 0.0016192478755449669, 0.0021826390999637678, 0.0020333272241588184, 0.0017169859383485905, 0.0022963752919523436, 0.0014547780514030796, 0.001738472841915389, 0.0015091893739319026, 0.0023854310266504179, 0.0022528527007060332, 0.0019903699147432868, 0.0018470256146551579, 0.0016771014064411939, 0.0016587645982505919, 0.0018488468252057786, 0.0018678961299563046, 0.0017962191885714826, 0.0017564104312074239, 0.0023974403349251473, 0.0017163983213859754, 0.0019722770278208768, 0.0019031061741556527, 0.0018235765021957665, 0.0024436226773295712, 0.0015853344492736967, 0.002033874805056532, 0.0021814801767512013, 0.0016252082965749256, 0.0018814557248820996, 0.001938226531875761, 0.002213295213138605, 0.001886338087299272, 0.0021457973262951732, 0.0014812702854878732, 0.0016756319044681845, 0.0020532473037723664, 0.0021834127492696091, 0.0020239509350241957, 0.0018816913196188934, 0.0017659370066927329, 0.0017410456489436441, 0.0022277805272735447, 0.0018844614794920867, 0.0015593163069363303, 0.001985876259239024, 0.0013813815013210362, 0.0022075332561276796, 0.0018391102669107189, 0.001861499776783535, 0.0017317814578335241, 0.0018125313316375261, 0.0019393337580201073], 3: [0.0011833006954896511, 0.001500017763137981, 0.0013458736849735922, 0.0014166935865776665, 0.0012364854228387584, 0.0015125426636008763, 0.0015066611035494664, 0.00139599649400386, 0.0017397137620757017, 0.0014195526471071035, 0.0013360802646941064, 0.0016386395306551855, 0.0014909377235186926, 0.0014112892800034558, 0.0014768602838281255, 0.0014131457067300195, 0.0014954697007517311, 0.0015224278485354867, 0.0013283413172935753, 0.0016299610703358381, 0.001361117568724922, 0.0015000910367611812, 0.0016263915899207173, 0.0014935748053959818, 0.0015791242007324488, 0.0013048698999805174, 0.0015058417492400228, 0.0013409621825394204, 0.001424056557251618, 0.0016574519163075833, 0.0014480796212239886, 0.0013374788640400394, 0.0015431883741736888, 0.00145355130176273, 0.0015967838085236567, 0.0012694342524317112, 0.0018154267670460957, 0.0015472331876473149, 0.0014838993734893528, 0.001490995915313187, 0.0013145855022858195, 0.001505759043643322, 0.0015278845049808431, 0.0013434275766364562, 0.0015723386509657476, 0.001541660048252015, 0.0015270766649718198, 0.0015595839453393565, 0.0013493503911159097, 0.0017295963218658679, 0.0015565717298414718, 0.0014919902077023919, 0.0016881215269307815, 0.0018522921323446844, 0.0013382592236762668, 0.0014324178513504459, 0.0012449023034546017, 0.0015168268956491072, 0.0020521189106260467, 0.0015824702551112332, 0.001353014957681936, 0.0014796144630778761, 0.0012850546224905961, 0.0013619918054756349, 0.0015340832731720669, 0.0013944229126058602, 0.0013175515776300227, 0.0016886686006131518, 0.0014907812771003098, 0.0017270689764322137, 0.0013894456653158168, 0.0014641072619173761, 0.0016041510406960263, 0.001528865641970998, 0.0014456983485184489, 0.001620394018395791, 0.0015050279047620692, 0.0014010236606367022, 0.0016554926035383102, 0.0012714512614179743, 0.0014953301433662056, 0.0013274869377774509, 0.0014712632217403301, 0.0012306459229558854, 0.00159405785760306, 0.0016537051647040773, 0.0011987949723353496, 0.0014055432146005819, 0.0014057448507313734, 0.0013477311208539393, 0.0013778214972806392, 0.0012832939057461055, 0.0013661142916265602, 0.0014510078037550537, 0.0017477239186663466, 0.001594500804993541, 0.0016401403055959188, 0.0014387213938066316, 0.0014527287889262394, 0.0016156812567841518, 0.0014071279393057506, 0.0015770122773329581, 0.0016926414990438129, 0.0013011718378879216, 0.0014408337883358197, 0.0015460603817031939, 0.0014210058682323624, 0.0016334201178603007, 0.0015765685787658555, 0.0013968948524897432, 0.0014222666043097736, 0.0014856712667357208, 0.0014433993083600768, 0.0014149493390825094, 0.0012701402352790114, 0.0014634155932179657, 0.0013603955244871808, 0.001403044030390405, 0.0014269305468205457, 0.001408922893291452, 0.0014283317378490875, 0.0013140816750925582, 0.0013670580413774169, 0.0012994240560613604, 0.0014599402740530882, 0.0014224178443989089, 0.0012215373660675027, 0.0014119071653637253, 0.0013003644264161385, 0.0015152145400582915, 0.0015296502855183429, 0.0015293486792450305, 0.0013059712313410589, 0.0014888021167870395, 0.0014100133535481052, 0.0018268705736495727, 0.001605914059600559, 0.0017481248091495223, 0.0014517325417320087, 0.0011394979569383308, 0.0015072598727397467, 0.0013184611471092683, 0.0015666469223721195, 0.0013896883401042165, 0.0014939696414551975, 0.0014146968622778364, 0.0013747075588467807, 0.0014853438320747761, 0.0012971841999637442, 0.0015182341231340503, 0.0014772140268698539, 0.0014704066471822456, 0.0017779307755193781, 0.0013672299617708845, 0.0014676174562941614, 0.001492247682325332, 0.0012746345727541303, 0.0015501096056513167, 0.0014776937370907161, 0.0016175087759508718, 0.0012802829155878432, 0.0014487387086843792, 0.001479194598684605, 0.0016434525314441189, 0.0015249080992212861, 0.0016278959902632768, 0.0012593609143941695, 0.0014737164456186537, 0.001694417881677791, 0.0013851902095793594, 0.0013333256487936629, 0.0013691178107948731, 0.0014737792773071873, 0.0017119698351523514, 0.0016003560886001277, 0.0018764937624961829, 0.0015253518081200725, 0.0012965411966189624, 0.0014760933582941371, 0.0014141122044178699, 0.001345822911270883, 0.0015121203441174095, 0.0015676059437327013, 0.0014134181990057013, 0.0013380479988620356, 0.0016219791307328772, 0.0012735153753244011, 0.0013030086184129472, 0.0015886753285080013, 0.0015026056382701118, 0.0014255020847785209, 0.0014253319393001452, 0.0013652138824115905, 0.0016187451375524469, 0.0012661634382621781, 0.0012836283954147886, 0.001607808567708695, 0.0013549646273962352, 0.0012705310558848476, 0.0015462403899504078], 4: [0.0013417345062128722, 0.0012856360390089553, 0.0013864596863075748, 0.0012442128598683169, 0.001312496294903535, 0.0014015705521893641, 0.0012123062564633282, 0.0014562654166482953, 0.0011587861436322591, 0.0012812098809136577, 0.0012233839716194776, 0.0012608137836121081, 0.001294559983616635, 0.0012385698453884039, 0.0013305518329605284, 0.0013095718526787274, 0.0012185003282734214, 0.0012608742787001584, 0.0013936576328013573, 0.0012113467987194607, 0.0011974285112792402, 0.0013647459703069837, 0.0012016318175573364, 0.001154987828665898, 0.001331360597144347, 0.0012049352615511463, 0.0013206001973301968, 0.0012783647310092388, 0.0013304865659944528, 0.0012523327790350844, 0.0012677306626808569, 0.0012873648946534124, 0.0013958186112659418, 0.0012191643984530397, 0.0012043951579101097, 0.0012518171815470935, 0.001424064763803324, 0.0013643211073563425, 0.0012432885900607847, 0.0012518501765636941, 0.0013110318643410677, 0.0012633501684675588, 0.0012054218478163912, 0.0013863347041703517, 0.0012642529552587527, 0.0013042948368703271, 0.0011943461157683719, 0.0012053601240161198, 0.0012183925239188853, 0.0012934449043191874, 0.0014134680746796466, 0.0012592794178206425, 0.0013350910188044155, 0.0011731727059424797, 0.0013538253363031233, 0.0013551705005919908, 0.0013597393466017313, 0.0012160386765775945, 0.0012925292157869104, 0.0012002422094812898, 0.0012352892485117952, 0.0011867458823435417, 0.0013340611955374034, 0.0013603810478265183, 0.0012167914396582769, 0.0012306058032123136, 0.0014295512014375108, 0.0012599758464266865, 0.0011959247272657764, 0.0012470664252288246, 0.0013037560784471807, 0.0013133177434725271, 0.0014420557549276343, 0.0012411620599295252, 0.0013018017282072658, 0.0013706210020666522, 0.0013403622582002887, 0.0012981060630268347, 0.0012282053117629768, 0.0013102444008777356, 0.0012180434189108273, 0.001206109840265865, 0.0012772030610222348, 0.0012523522926488791, 0.0012043253890774977, 0.0012146805669560306, 0.0012152131852539133, 0.0013074898393348364, 0.0012352112206085817, 0.0011948237200965659, 0.0012541197148379511, 0.0012418407085657466, 0.0011995362875008042, 0.0012619915176905739, 0.0012232489651266148, 0.0012122785022718355, 0.0011876210890215066, 0.0012539112884904283, 0.0013261786442492008, 0.0013118314981286888, 0.0012972649133712938, 0.0012563206664105162, 0.0012972762140917346, 0.0013910618320832546, 0.0013129154587146676, 0.0013440279962838608, 0.0012892306750135574, 0.0013904220325399977, 0.0012241742474562768, 0.0012927483067693656, 0.0013178360224286334, 0.0013301324718502461, 0.00137463972167454, 0.0012676214767544098, 0.0013191557130198485, 0.0012397006891869998, 0.0012350434574368828, 0.0011986013176205632, 0.0011576734705941727, 0.0013045513168017133, 0.0011386959984602963, 0.0012639174670186959, 0.0012143291629425031, 0.0012862581302672039, 0.0013553327792726773, 0.0012422871454906301, 0.0013306866212199032, 0.0012244021376362942, 0.0013330025707385836, 0.0012956009001398355, 0.001247080313988055, 0.0012916144198414645, 0.0013083021487850172, 0.0013353388697109846, 0.0012421134837824327, 0.0012795850291427778, 0.0011855772772404786, 0.001268466891670317, 0.0012159975437486192, 0.00124495020308077, 0.0012479036526934087, 0.0011926690953252056, 0.001268986285555343, 0.0012935868049464983, 0.0012658776808288461, 0.0013095983318477228, 0.0012605240327627121, 0.0012427006639598149, 0.0013400843377194881, 0.0012749930548068094, 0.0011330248385058609, 0.0012310151919296937, 0.0012057566351488156, 0.0013145518552617975, 0.0011488910702222056, 0.0012431605572717915, 0.0012409991798384037, 0.0014469779690758186, 0.0012478012233681423, 0.0012420289537875451, 0.0013100764372512425, 0.0011641482118325826, 0.0012943776504235516, 0.0011939468884370448, 0.0012342518274403402, 0.0011716557741500357, 0.0014171367141597013, 0.0012304489233696775, 0.0013384717570219015, 0.0012319349863322594, 0.0012674259929738023, 0.0013856395440777912, 0.0012452513157513298, 0.0013293216845795444, 0.0012793560495927164, 0.0012161462943473035, 0.0012493024776661296, 0.0011588229731547542, 0.001232682085249015, 0.0013501018917745247, 0.0011570026766315039, 0.0013089144644231681, 0.0012592347404701181, 0.0013412724721408986, 0.0011700766349356587, 0.0012705795857774608, 0.0013018391718705973, 0.0012920638998352013, 0.0012970568169007189, 0.0013595700096371371, 0.0013469396391004149, 0.0011858931308071099, 0.0013015404899842044, 0.0011654306722908808, 0.0013516940870974827, 0.0012573996854084554, 0.0013456504961618897, 0.0012077465228941425, 0.0012941957056867806, 0.0012097306583740417], 5: [0.0011630521818178062, 0.0011318026540214307, 0.0011544630537104559, 0.0011698303179845462, 0.0012566173330460135, 0.0012429092054788422, 0.0012002923546729314, 0.0012437563478281837, 0.0011729237277998453, 0.0011728425781441791, 0.0010886534540905967, 0.0011565110158748551, 0.0011670477722469182, 0.0011892521311381147, 0.0011127591052113117, 0.0011110621388668962, 0.0010789295484009123, 0.0011924593844391427, 0.0011354324826823501, 0.0011549361011866043, 0.0012243726770548773, 0.0011673250953060971, 0.0010632051553148348, 0.0011803864581268527, 0.0011335364238165911, 0.0011082358523394668, 0.0011701999804294913, 0.0012221447507679117, 0.0012464274727523926, 0.0012216879465653521, 0.0011113754143373402, 0.0011808193067875923, 0.001170150702531997, 0.0011708422968851374, 0.0012044110777945038, 0.0011677625919001829, 0.0011665058300894225, 0.0011506847087777729, 0.0011365845002660972, 0.0011273885640008037, 0.0011896942474936155, 0.0011907350891572296, 0.0011405955007606735, 0.001178360396764214, 0.0011756960252841267, 0.0012392597069716071, 0.0012220773138409869, 0.0012010346332307293, 0.0011648851483025621, 0.0012045202366733824, 0.0011513434225619602, 0.001250145194887618, 0.0011820470487920508, 0.0011069178409682354, 0.0011607336320230662, 0.0011328022974503528, 0.0011751681766670053, 0.0011396421683973188, 0.0012079337451385095, 0.0012393473551036802, 0.0012624222652858046, 0.0011444103430904447, 0.0011275233841788054, 0.0011379182086893762, 0.0012122249927509674, 0.0011238796078907537, 0.0011907340199606147, 0.0011797548969678599, 0.0011351043936026206, 0.0011353337079547075, 0.0011979088542792228, 0.0011762287422129928, 0.0012083224183076803, 0.0011944837741492856, 0.0011617332079504694, 0.001142682278630077, 0.0011532319247032523, 0.001259017512915107, 0.0012105079380293712, 0.0012325483848837841, 0.0011475837750876486, 0.0011762026510163527, 0.0012076452814418874, 0.0011938911972255088, 0.0011831052817163545, 0.0011485070473839323, 0.001175068842000312, 0.001131403350349443, 0.0011333140928423365, 0.0011358085447494642, 0.0011625246616214198, 0.0010894687538758416, 0.0011057699548723818, 0.0011371472533597386, 0.0011473437046325731, 0.0011612819784886383, 0.0011392465727268141, 0.0011820502044128695, 0.0011541642167582364, 0.0011531343938452228, 0.0011331860625155418, 0.001157820692988813, 0.0011365389791098781, 0.0012032058585149897, 0.001240914020968003, 0.001232562113015838, 0.0011908763645714403, 0.0011146657121705052, 0.0011932740182967493, 0.0011602082648401444, 0.0011417351562347279, 0.0011731762926564726, 0.0011557975292857186, 0.0011536797611915923, 0.0011982833797856047, 0.0011398703671609608, 0.0011434045248500996, 0.0012507674554958635, 0.0011508544654985917, 0.0011553340913336961, 0.0012481522812966949, 0.0011660528311574149, 0.0012128802974362726, 0.0011370172047122707, 0.001184895381920883, 0.0011475486526486335, 0.0011190913948275693, 0.0011127069976742425, 0.0011388334771172009, 0.0011593172637260705, 0.0011783792323161856, 0.0011531057088382082, 0.0011249585783692589, 0.001185679275190032, 0.0012178748966377114, 0.0011430912147598565, 0.0011701210847697907, 0.001168350139482991, 0.0012004099529990887, 0.0012196253265307393, 0.0012510672766622983, 0.001175224081402383, 0.0012648591386294446, 0.0011014547590456813, 0.0011674282866860144, 0.0011669998527805928, 0.0011089489016117777, 0.0011617723561059086, 0.0011473975698125158, 0.0011883297111784127, 0.0011744709819259394, 0.0011777534165106269, 0.0011450110700195585, 0.0011955416973849589, 0.0012047826257775972, 0.0011138492735326708, 0.0011227056348778893, 0.0011700168480407688, 0.0012037362244012439, 0.001304963832064112, 0.0011861960498551023, 0.001191730068559221, 0.0012239798357741596, 0.0012047693141638246, 0.0011965135585672551, 0.0011671082937371445, 0.0011732858107813191, 0.0011547481514757739, 0.0011530977053391825, 0.0011539757003313426, 0.0011542076262559681, 0.0012086089042733579, 0.0011794228594797812, 0.0011470415948823586, 0.0011707774952644879, 0.0011917438846069353, 0.0011956976867369359, 0.0011382416144315114, 0.0011576540262037706, 0.0011912010989725186, 0.0011145709765452033, 0.0011846408555556201, 0.0011636235388581394, 0.0011875551904640979, 0.0011217561351849841, 0.0011993277692234662, 0.0012841369581242004, 0.0011387084735990578, 0.0010841115257536947, 0.0012081562429841655, 0.0011867777915214616, 0.0011810792376812558, 0.0012189261481334169, 0.0012418853169744214, 0.0012284027754687424, 0.0011076048746496933, 0.0012249093469402848, 0.0011925421083870975, 0.0011423990713806415, 0.0011548522047361339], 6: [0.0011235945614335007, 0.0011145135648582541, 0.0011404086832264202, 0.0011168589981273803, 0.0010906233386155522, 0.0011049794687862065, 0.0011423703956561909, 0.0011343060181090328, 0.0011377689665093113, 0.0011658858905303755, 0.0011536959885414082, 0.0011414595859869407, 0.00111560207248024, 0.0011149523072501544, 0.0011119227837289801, 0.0011278955657787111, 0.0011147455205755654, 0.0010955771715561106, 0.0011915954252778537, 0.0011099567541445234, 0.0011008804635568377, 0.0011284115762722267, 0.0011289285763657468, 0.0011018555974064244, 0.0011415272787471714, 0.0011290033403436178, 0.001113035983730117, 0.0011209016337543416, 0.0010901643492609949, 0.001084254415790767, 0.0011664409312088489, 0.0011421531448891548, 0.0011159615097542509, 0.001128202012221432, 0.0011295566008515466, 0.0011521935804720887, 0.0011418119979218278, 0.0011100229797288085, 0.001097935752339208, 0.001105928378729124, 0.0011440404825542928, 0.001121993407191496, 0.0011304483627858411, 0.0011441813840469351, 0.0011161686588985096, 0.0011163007245642371, 0.001120538679694456, 0.0011211471087256982, 0.0011013171080822592, 0.0011584295246869704, 0.0011137188458981116, 0.0011173763986432389, 0.0011211871315726422, 0.0011489817229922493, 0.0011550927267029959, 0.0011505312071498229, 0.0011226918394966646, 0.0010938692752109286, 0.001105321984841515, 0.0011431692254877247, 0.0011010941109917808, 0.0010988730062682687, 0.0011414101511201056, 0.0011300811536847448, 0.0011200469768122329, 0.0011112430122348722, 0.0011101847373160478, 0.0011007528093483205, 0.0011042885570523215, 0.0011316441271622363, 0.001099949019524069, 0.0011531806684022671, 0.0011091455251075024, 0.0011416075528739972, 0.0010976690266598936, 0.0010969082677938526, 0.0011409614776705474, 0.0011075244159518068, 0.0011504279717210735, 0.0011382418269754006, 0.0011076725947293299, 0.0011147724193935138, 0.0011082378927780954, 0.0011385170434003616, 0.0011270838253603878, 0.0011530911764918651, 0.0011074173583533996, 0.0011095447774147581, 0.0011055260305696097, 0.0011351881243931794, 0.0010877369415963627, 0.0011157056452473302, 0.0011575184123524891, 0.0011386672353656717, 0.0011417823922398882, 0.0011105601296059506, 0.0011059177559014303, 0.0011297323163945037, 0.0011069443494317217, 0.0011223121424949072, 0.0011495127921889816, 0.0011276516588197573, 0.0010897816829772405, 0.0011679168507342484, 0.0011141801843180068, 0.0011344079690059636, 0.0011391638839845829, 0.0011367298161643596, 0.0011240502609504177, 0.0011595631740183865, 0.00111236511801775, 0.0011088228550705298, 0.001104639385096606, 0.0011325149426615363, 0.001113860824173142, 0.0011145074869257381, 0.0011079439682537091, 0.0011113367918609434, 0.0011186895485037524, 0.0011177571516510182, 0.0010915755611050739, 0.0011170933617631472, 0.001153736632284724, 0.0011147124317727936, 0.0011379718613372825, 0.0010966715684233119, 0.0011300381314940484, 0.0011280104271199252, 0.0011088350167772288, 0.0011359550125198865, 0.001118035734471789, 0.0011252291980015695, 0.0011313469380915785, 0.0011026450299050463, 0.0011107718871879477, 0.0011302237404027578, 0.0011489690053300991, 0.0011086346943368968, 0.0011136848101317019, 0.0011230540090092773, 0.0011399458418134413, 0.0011026277045014098, 0.0011446483930709669, 0.0011098947607304965, 0.0011336579956468002, 0.0011162244783714161, 0.0011021618212115494, 0.0011281264177245857, 0.001124220132969451, 0.0011179572051782923, 0.0011231047321753312, 0.0011081960884352007, 0.0011077814385737075, 0.0011029645011360458, 0.0010945539090745997, 0.0011007079912919461, 0.0011462835025253006, 0.0011247647764386283, 0.0010955511547033766, 0.0011331448661869295, 0.0011074223285814312, 0.0011010278524377509, 0.0011297447336538504, 0.0011288052291304586, 0.0011098745836345769, 0.0011202538832931054, 0.0011420287901687074, 0.0011124635086748919, 0.0011391288201679178, 0.0011217957581949969, 0.0011145593107672043, 0.0010809632437350379, 0.001107053179691712, 0.0011280538460325239, 0.0011064955212466098, 0.0011620845662671942, 0.0011210222021346078, 0.0011367262599594116, 0.0011124269696379131, 0.0010872862975309669, 0.0011104214516119636, 0.0011235186419542007, 0.001137703249805488, 0.0010870108777411192, 0.0011379937816261105, 0.0011193634212135965, 0.0011301900611222506, 0.0011681187497783409, 0.0011077151777285224, 0.0011145207717655385, 0.0010948448357871322, 0.0011320879868171507, 0.0010948028816133016, 0.0011475731027684711, 0.0011080475179617846, 0.0011207877095269791, 0.0011250734672372642, 0.0011431142063766436, 0.0011211606072807237, 0.001097224215829857]}\n"
     ]
    }
   ],
   "source": [
    "parameters_distributions = {\n",
    "    \"max_number_inputs\": 7,\n",
    "    \"min_number_inputs\": 1,\n",
    "    \"number_of_states\": 5,\n",
    "    \"number_of_distributions\": 200\n",
    "}\n",
    "generator = generate_input_and_conditional_output(\n",
    "    'entropy_0.75', parameters_distributions, cond_output_type=\"dirichlet\"\n",
    ")\n",
    "\n",
    "#set new file name before turning run on\n",
    "RUN = False\n",
    "DATA_PATH = \"data_experiments/\"\n",
    "FILE_NAME = DATA_PATH + \"max_impact_individual_nudges_entropy75_first200samples.json\"\n",
    "if RUN:\n",
    "    max_individual_impact_dict = {}\n",
    "    prev_number_of_vars = -1\n",
    "    for count, dist_dict in enumerate(generator):\n",
    "        print(count)\n",
    "        number_of_vars = dist_dict[\"number_of_vars\"]\n",
    "        number_of_states = dist_dict[\"number_of_states\"]\n",
    "        if number_of_vars != len(dist_dict[\"input_dist\"].shape):\n",
    "            print(\"WARNING in sample {} input dist has weird distribution\".format(count))\n",
    "        if number_of_vars != prev_number_of_vars:\n",
    "            prev_number_of_vars = number_of_vars \n",
    "            print(\"the number of vars {}\".format(number_of_vars))\n",
    "        #print(dist_dict[\"cond_output\"].shape)\n",
    "        max_impact = find_max_impact_individual_nudge_exactly(\n",
    "            dist_dict[\"input_dist\"], dist_dict[\"cond_output\"], NUDGE_SIZE/2,\n",
    "            without_conditional=True\n",
    "        )\n",
    "        if number_of_vars in max_individual_impact_dict:\n",
    "            max_individual_impact_dict[number_of_vars].append(max_impact)\n",
    "        else:\n",
    "            max_individual_impact_dict[number_of_vars] = [max_impact]\n",
    "\n",
    "        if (count+1)%5==0 and count != 0:\n",
    "            with open(FILE_NAME, 'w') as f:\n",
    "                json.dump(max_individual_impact_dict, f, indent=4)\n",
    "\n",
    "    with open(FILE_NAME, 'w') as f:\n",
    "        json.dump(max_individual_impact_dict, f, indent=4)\n",
    "\n",
    "    print(max_individual_impact_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the maximum global nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, max_global_nudge_impact = maximum_nudges.find_max_control_impact(input_dist, cond_output, NUDGE_SIZE/2)\n",
    "print(max_global_nudge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_distributions = {\n",
    "    \"max_number_inputs\": 7,\n",
    "    \"min_number_inputs\": 2,\n",
    "    \"number_of_states\": 5,\n",
    "    \"number_of_distributions\": 200\n",
    "}\n",
    "\n",
    "generator = generate_input_and_conditional_output(\n",
    "    'dirichlet', parameters_distributions, cond_output_type=\"dirichlet\"\n",
    ")\n",
    "\n",
    "#change file name before turning RUN on\n",
    "RUN = False\n",
    "FILE_NAME = \"max_impact_global_nudges_dirichlet_first200samples2.json\"\n",
    "if RUN:\n",
    "    max_global_impact_dict = {}\n",
    "    prev_number_of_vars = -1\n",
    "    for count, dist_dict in enumerate(generator):\n",
    "        print(count)\n",
    "        number_of_vars = dist_dict[\"number_of_vars\"]\n",
    "        number_of_states = dist_dict[\"number_of_states\"]\n",
    "        if number_of_vars != len(dist_dict[\"input_dist\"].shape):\n",
    "            print(\"WARNING in sample {} input dist has weird distribution\".format(count))\n",
    "        if number_of_vars != prev_number_of_vars:\n",
    "            prev_number_of_vars = number_of_vars \n",
    "            print(\"the number of vars {}\".format(number_of_vars))\n",
    "        #print(dist_dict[\"cond_output\"].shape)\n",
    "        _, _, max_global_nudge_impact = maximum_nudges.find_max_control_impact(\n",
    "            dist_dict[\"input_dist\"], dist_dict[\"cond_output\"], NUDGE_SIZE/2\n",
    "        )\n",
    "\n",
    "        if number_of_vars in max_global_impact_dict:\n",
    "            max_global_impact_dict[number_of_vars].append(max_global_nudge_impact)\n",
    "        else:\n",
    "            max_global_impact_dict[number_of_vars] = [max_global_nudge_impact]\n",
    "\n",
    "        if (count+1)%5==0 and count != 0:\n",
    "            with open(FILE_NAME, 'w') as f:\n",
    "                json.dump(max_global_impact_dict, f, indent=4)\n",
    "\n",
    "    with open(FILE_NAME, 'w') as f:\n",
    "        json.dump(max_global_impact_dict, f, indent=4)\n",
    "\n",
    "    print(max_global_impact_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution with 75 percent of the maximum entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_distributions = {\n",
    "    \"max_number_inputs\": 7,\n",
    "    \"min_number_inputs\": 2,\n",
    "    \"number_of_states\": 5,\n",
    "    \"number_of_distributions\": 200\n",
    "}\n",
    "\n",
    "generator = generate_input_and_conditional_output(\n",
    "    'entropy_0.75', parameters_distributions, cond_output_type=\"dirichlet\"\n",
    ")\n",
    "\n",
    "#change file name before turning RUN on\n",
    "RUN = False\n",
    "DATA_PATH = \"data_experiments\"\n",
    "FILE_NAME = DATA_PATH + \"max_impact_global_nudges_entropy75_first200samples.json\"\n",
    "if RUN:\n",
    "    max_global_impact_dict = {}\n",
    "    prev_number_of_vars = -1\n",
    "    for count, dist_dict in enumerate(generator):\n",
    "        print(count)\n",
    "        number_of_vars = dist_dict[\"number_of_vars\"]\n",
    "        number_of_states = dist_dict[\"number_of_states\"]\n",
    "        if number_of_vars != len(dist_dict[\"input_dist\"].shape):\n",
    "            print(\"WARNING in sample {} input dist has weird distribution\".format(count))\n",
    "        if number_of_vars != prev_number_of_vars:\n",
    "            prev_number_of_vars = number_of_vars \n",
    "            print(\"the number of vars {}\".format(number_of_vars))\n",
    "        #print(dist_dict[\"cond_output\"].shape)\n",
    "        _, _, max_global_nudge_impact = maximum_nudges.find_max_control_impact(\n",
    "            dist_dict[\"input_dist\"], dist_dict[\"cond_output\"], NUDGE_SIZE/2\n",
    "        )\n",
    "\n",
    "        if number_of_vars in max_global_impact_dict:\n",
    "            max_global_impact_dict[number_of_vars].append(max_global_nudge_impact)\n",
    "        else:\n",
    "            max_global_impact_dict[number_of_vars] = [max_global_nudge_impact]\n",
    "\n",
    "        if (count+1)%5==0 and count != 0:\n",
    "            with open(FILE_NAME, 'w') as f:\n",
    "                json.dump(max_global_impact_dict, f, indent=4)\n",
    "\n",
    "    with open(FILE_NAME, 'w') as f:\n",
    "        json.dump(max_global_impact_dict, f, indent=4)\n",
    "\n",
    "    print(max_global_impact_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
