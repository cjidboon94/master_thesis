{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data from several files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import itertools\n",
    "from simulate import find_mean_std_mse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "\n",
    "#play with this for better figure sizes \n",
    "mpl.rcParams['figure.figsize'] = (7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"data_experiments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_percentile_confidence_bounds(values):\n",
    "    return np.percentile(values, 10), np.median(values), np.percentile(values, 90)\n",
    "    \n",
    "print(get_percentile_confidence_bounds([1, 10, 3, 80, 4, 5, 8, 12, 13, 4, 7]))\n",
    "\n",
    "def plot_mean_and_confidence(plot_range, mean, mean_label, confidence_interval, \n",
    "                             confidence_interval_title):\n",
    "    \"\"\"\n",
    "    Plot the mean and some kind of confidence interval (standard deviation or\n",
    "    mean-squared-error)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    plot_range: iterable\n",
    "    mean: an iterable\n",
    "        the mean of the values at that point\n",
    "    confidence_interval: an iterable\n",
    "        Representing the  interval of confidence in that point. \n",
    "        The iterable should have length plot_range.\n",
    "    confidence_interval_title: a string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lower_bound = np.array(mean)-np.array(confidence_interval)\n",
    "    upper_bound = np.array(mean)+np.array(confidence_interval)\n",
    "    plt.plot(plot_range, mean, label=mean_label)\n",
    "    if confidence_interval_title:\n",
    "        plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                         label='{}'.format(confidence_interval_title),\n",
    "                         alpha=0.2)\n",
    "    else:\n",
    "        plt.fill_between(plot_range, lower_bound, upper_bound, alpha=0.2)\n",
    "        \n",
    "        \n",
    "def plot_results(*args, **kwargs):\n",
    "    \"\"\"plot results from simulations\n",
    "    \n",
    "    Parameters:\n",
    "        args: 1 or more dicts. The dicts should have for the keys numerical\n",
    "            input values and for the values iterables of numbers.\n",
    "        kwargs: at least the arguments xlabel, ylabel, title\n",
    "        \n",
    "    \"\"\"\n",
    "    for argument in args:\n",
    "        data, meta_dict = argument\n",
    "        variable_range, mean, std, batches_std = (\n",
    "            find_mean_std_mse(data, 10)\n",
    "        )\n",
    "        \n",
    "        if kwargs['std_of_batches']: \n",
    "            plot_mean_and_confidence(variable_range, mean, meta_dict['mean_label'], \n",
    "                                     batches_std, \"batches stdev\")\n",
    "        else:\n",
    "            plot_mean_and_confidence(variable_range, mean, meta_dict['mean_label'], \n",
    "                                     std, \"stdev\")\n",
    "    \n",
    "    plt.xlabel(kwargs['xlabel'])\n",
    "    plt.ylabel(kwargs['ylabel'])\n",
    "    plt.legend()\n",
    "    plt.title(kwargs['title'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data for the maximum nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_keys_from_string_to_int(dictionary):\n",
    "    return {int(k):v for k,v in dictionary.items()}\n",
    "\n",
    "def files_to_dict(file_names):\n",
    "    \"\"\" Load the data from the files and make keys into ints\"\"\"\n",
    "    dictionaries = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as f:\n",
    "            dictionaries.append(json.load(f))\n",
    "\n",
    "    dictionaries = [dict_keys_from_string_to_int(i) for i in dictionaries]\n",
    "    return dictionaries\n",
    "\n",
    "def compare_settings(value_to_scores, min_value, max_value):\n",
    "    value_to_average_scores = {}\n",
    "    for i in range(min_value, max_value+1, 1):\n",
    "        average_impacts = []\n",
    "        for count, dirichlet_dict in enumerate(value_to_scores):\n",
    "            try:\n",
    "                average_impacts.append(np.mean(dirichlet_dict[i]))\n",
    "            except KeyError:\n",
    "                print(\"the {} file has variable {} missing\".format(i, count))\n",
    "\n",
    "        value_to_average_scores[i] = average_impacts\n",
    "\n",
    "    return value_to_average_scores\n",
    "\n",
    "def flip_sign_values_dict(dictionary):\n",
    "    for key, values in dictionary.items():\n",
    "        dictionary[key] = [-value for value in values]\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "def find_max_scores_per_value(dictionaries, min_value, max_value):\n",
    "    value_to_maximum_scores = {}\n",
    "    for i in range(min_value, max_value+1, 1):\n",
    "        impacts = []\n",
    "        for count, dictionary in enumerate(dictionaries):\n",
    "            try:\n",
    "                impacts.append(dictionary[i])\n",
    "            except KeyError:\n",
    "                print(\"the {} file has variable {} missing\".format(i, count))\n",
    "\n",
    "        max_impacts = [max(scores) for scores in list(zip(*impacts))]\n",
    "        value_to_maximum_scores[i] = max_impacts\n",
    "\n",
    "    return value_to_maximum_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### get maximum nudge impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_impacts(number_of_vars, number_of_states, percentage_max_entropy_size, \n",
    "                    nudge_type, dist_start, dist_end):\n",
    "    filename_to_save_impacts =  \"impacts_{}var_{}states_{}entropy_{}_nudge_dists{}-{}.json\".format(\n",
    "        number_of_vars, number_of_states, percentage_max_entropy_size, nudge_type, dist_start, dist_end\n",
    "    )\n",
    "    with open(\"data_experiments/\" + filename_to_save_impacts, 'r') as f:\n",
    "        impacts = json.load(f)\n",
    "        \n",
    "    return impacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "NUMBER_OF_STATES = 3\n",
    "PERCENTAGE_MAX_ENTROPY_SIZE = 75\n",
    "nudge_types = [\"individual\", \"local\", \"synergistic\", \"global\"]\n",
    "numbers_of_var = [2, 3, 4, 5, 6, 7]\n",
    "for nudge_type in nudge_types:\n",
    "    print(nudge_type)\n",
    "    var_to_impacts = {}\n",
    "    for number_of_var in numbers_of_var:\n",
    "        print(number_of_var)\n",
    "        if nudge_type == \"synergistic\" and number_of_var==2:\n",
    "            continue\n",
    "        if nudge_type == \"local\" and number_of_var==2:\n",
    "            impacts = get_max_impacts(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                \"individual\", dist_start=100, dist_end=200\n",
    "            )\n",
    "        else:\n",
    "            impacts = get_max_impacts(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                nudge_type, dist_start=100, dist_end=200\n",
    "            )\n",
    "\n",
    "            if nudge_type == \"local\" or nudge_type == \"synergistic\":\n",
    "                impacts = [-1*impact for impact in impacts]\n",
    "                \n",
    "        \n",
    "#         print(stats.shapiro(impacts))\n",
    "#         print(stats.normaltest(impacts))\n",
    "#         sns.distplot(impacts)\n",
    "#         plt.show()\n",
    "        \n",
    "        var_to_impacts[number_of_var] = impacts\n",
    "        \n",
    "    variable_range, mean, std, batches_std = (\n",
    "        find_mean_std_mse(var_to_impacts, 10)\n",
    "    )\n",
    "    \n",
    "    std2 = list(2*np.array(std))\n",
    "    #plot_mean_and_confidence(variable_range, mean, nudge_type, std2, None)\n",
    "    \n",
    "    #add confidence intervals based on percentiles\n",
    "    lower_bound, median, upper_bound = [], [], []\n",
    "    for variable_number, impacts in var_to_impacts.items():\n",
    "        lower_bound.append(np.percentile(impacts, 2.5))\n",
    "        median.append(np.median(impacts)) \n",
    "        upper_bound.append(np.percentile(impacts, 97.5))\n",
    "        \n",
    "    plot_range = list(var_to_impacts.keys())\n",
    "    plt.plot(plot_range, median, label=nudge_type)\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(\"95%\"), alpha=0.2)\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"intervention impact\")\n",
    "plt.title(\"max impacts for distribution with {}% of the max entropy\".format(PERCENTAGE_MAX_ENTROPY_SIZE))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate results maximum nudge impacts using a different process to generate the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_STATES = 3\n",
    "PERCENTAGE_MAX_ENTROPY_SIZE = 90\n",
    "nudge_types = [\"individual\", \"local\", \"synergistic\", \"global\"]\n",
    "numbers_of_var = [2, 3, 4, 5, 6, 7]\n",
    "for nudge_type in nudge_types:\n",
    "    print(nudge_type)\n",
    "    var_to_impacts = {}\n",
    "    for number_of_var in numbers_of_var:\n",
    "        print(number_of_var)\n",
    "        if nudge_type == \"synergistic\" and number_of_var==2:\n",
    "            continue\n",
    "        if nudge_type == \"local\" and number_of_var==2:\n",
    "            impacts = get_max_impacts(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                \"individual\", dist_start=200, dist_end=300\n",
    "            )\n",
    "        else:\n",
    "            impacts = get_max_impacts(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                nudge_type, dist_start=200, dist_end=300\n",
    "            )\n",
    "\n",
    "            if nudge_type == \"local\" or nudge_type == \"synergistic\":\n",
    "                impacts = [-1*impact for impact in impacts]\n",
    "\n",
    "        var_to_impacts[number_of_var] = impacts\n",
    "        \n",
    "        #testing for normality \n",
    "#         print(stats.shapiro(impacts))\n",
    "#         print(stats.normaltest(impacts))\n",
    "#         sns.distplot(impacts)\n",
    "#         plt.show()\n",
    "        \n",
    "    variable_range, mean, std, batches_std = (\n",
    "        find_mean_std_mse(var_to_impacts, 10)\n",
    "    )\n",
    "    std2 = list(2*np.array(std))\n",
    "    #plot_mean_and_confidence(variable_range, mean, nudge_type, std2, None)\n",
    "    \n",
    "    #add confidence intervals based on percentiles\n",
    "    lower_bound, median, upper_bound = [], [], []\n",
    "    for variable_number, impacts in var_to_impacts.items():\n",
    "        lower_bound.append(np.percentile(impacts, 2.5))\n",
    "        median.append(np.median(impacts)) \n",
    "        upper_bound.append(np.percentile(impacts, 97.5))\n",
    "        \n",
    "    plot_range = list(var_to_impacts.keys())\n",
    "    plt.plot(plot_range, median, label=nudge_type)\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(\"95%\"), alpha=0.2)\n",
    "    \n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"intervention impact\")\n",
    "plt.title(\"max impacts for distribution with {}% of the max entropy\".format(PERCENTAGE_MAX_ENTROPY_SIZE))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random nudge impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_impacts(number_of_vars, number_of_states, percentage_max_entropy_size, \n",
    "                       nudge_type, dist_start, dist_end):\n",
    "    filename_to_save_impacts =  \"random_impacts_{}var_{}states_{}entropy_{}_nudge_dists{}-{}.json\".format(\n",
    "        number_of_vars, number_of_states, percentage_max_entropy_size, nudge_type, dist_start, dist_end \n",
    "    )\n",
    "    with open(\"data_experiments/\" + filename_to_save_impacts, 'r') as f:\n",
    "        impacts = json.load(f)\n",
    "        \n",
    "    return impacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_STATES = 3\n",
    "DIST_START, DIST_END = 100, 200\n",
    "PERCENTAGE_MAX_ENTROPY_SIZE = 75\n",
    "\n",
    "nudge_types = [\"individual\", \"focused\", \"local\", \"synergistic\", \"global\"]\n",
    "#nudge_types = [\"individual\", \"local\", \"synergistic\", \"global\"]\n",
    "#nudge_types = [\"local\", \"synergistic\", \"global\"]\n",
    "#nudge_types = [\"synergistic\", \"global\"]\n",
    "\n",
    "numbers_of_var = [2, 3, 4, 5, 6, 7]\n",
    "for nudge_type in nudge_types:\n",
    "    print(nudge_type)\n",
    "    var_to_impacts = {}\n",
    "    for number_of_var in numbers_of_var:\n",
    "        print(number_of_var)\n",
    "        if nudge_type == \"synergistic\" and number_of_var==2:\n",
    "            continue\n",
    "        if (nudge_type == \"local\" or nudge_type == \"global\") and number_of_var==2:\n",
    "            impacts = get_random_impacts(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                \"individual\", DIST_START, DIST_END\n",
    "            )\n",
    "        else:\n",
    "            impacts = get_random_impacts(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                nudge_type, DIST_START, DIST_END\n",
    "            )\n",
    "\n",
    "        var_to_impacts[number_of_var] = impacts\n",
    "        \n",
    "#         #testing for normality \n",
    "#         print(stats.shapiro(impacts))\n",
    "#         print(stats.normaltest(impacts))\n",
    "#         sns.distplot(impacts)\n",
    "#         plt.show()\n",
    "        \n",
    "    variable_range, mean, std, batches_std = (\n",
    "        find_mean_std_mse(var_to_impacts, 10)\n",
    "    )\n",
    "    std2 = list(2*np.array(std))\n",
    "#     plot_mean_and_confidence(variable_range, mean, nudge_type, std2, None)\n",
    "    \n",
    "    #add confidence intervals based on percentiles\n",
    "    lower_bound, median, upper_bound = [], [], []\n",
    "    for variable_number, impacts in var_to_impacts.items():\n",
    "        lower_bound.append(np.percentile(impacts, 2.5))\n",
    "        median.append(np.median(impacts)) \n",
    "        upper_bound.append(np.percentile(impacts, 97.5))\n",
    "        \n",
    "    plot_range = list(var_to_impacts.keys())\n",
    "    plt.plot(plot_range, median, label=nudge_type)\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(\"95%\"), alpha=0.2)\n",
    "\n",
    "    \n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"intervention impact\")\n",
    "plt.title(\"impact of random interventions for {}% of the max entropy\".format(PERCENTAGE_MAX_ENTROPY_SIZE))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_impacts_validate(number_of_vars, number_of_states, percentage_max_entropy_size, \n",
    "                       nudge_type, dist_start, dist_end):\n",
    "    \n",
    "    filename_to_save_impacts =  \"random_impacts_validate_{}var_{}states_{}entropy_{}_nudge_dists{}-{}.json\".format(\n",
    "        number_of_vars, number_of_states, percentage_max_entropy_size, nudge_type, dist_start, dist_end \n",
    "    )\n",
    "    with open(\"data_experiments/\" + filename_to_save_impacts, 'r') as f:\n",
    "        impacts = json.load(f)\n",
    "        \n",
    "    return impacts\n",
    "\n",
    "NUMBER_OF_STATES = 3\n",
    "DIST_START, DIST_END = 200, 300\n",
    "PERCENTAGE_MAX_ENTROPY_SIZE = 75\n",
    "\n",
    "nudge_types = [\"individual\", \"focused\", \"local\", \"synergistic\", \"global\"]\n",
    "#nudge_types = [\"individual\", \"local\", \"synergistic\", \"global\"]\n",
    "#nudge_types = [\"local\", \"synergistic\", \"global\"]\n",
    "#nudge_types = [\"synergistic\", \"global\"]\n",
    "\n",
    "numbers_of_var = [2, 3, 4, 5, 6, 7]\n",
    "for nudge_type in nudge_types:\n",
    "    print(nudge_type)\n",
    "    var_to_impacts = {}\n",
    "    for number_of_var in numbers_of_var:\n",
    "        print(number_of_var)\n",
    "        if nudge_type == \"synergistic\" and number_of_var==2:\n",
    "            continue\n",
    "        if (nudge_type == \"local\" or nudge_type == \"global\") and number_of_var==2:\n",
    "            impacts = get_random_impacts_validate(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                \"individual\", DIST_START, DIST_END\n",
    "            )\n",
    "        else:\n",
    "            impacts = get_random_impacts_validate(\n",
    "                number_of_var, NUMBER_OF_STATES, PERCENTAGE_MAX_ENTROPY_SIZE, \n",
    "                nudge_type, DIST_START, DIST_END\n",
    "            )\n",
    "\n",
    "        var_to_impacts[number_of_var] = impacts\n",
    "        \n",
    "#         #testing for normality \n",
    "#         print(stats.shapiro(impacts))\n",
    "#         print(stats.normaltest(impacts))\n",
    "#         sns.distplot(impacts)\n",
    "#         plt.show()\n",
    "        \n",
    "    variable_range, mean, std, batches_std = (\n",
    "        find_mean_std_mse(var_to_impacts, 10)\n",
    "    )\n",
    "    std2 = list(2*np.array(std))\n",
    "#     plot_mean_and_confidence(variable_range, mean, nudge_type, std2, None)\n",
    "    \n",
    "    #add confidence intervals based on percentiles\n",
    "    lower_bound, median, upper_bound = [], [], []\n",
    "    for variable_number, impacts in var_to_impacts.items():\n",
    "        lower_bound.append(np.percentile(impacts, 2.5))\n",
    "        median.append(np.median(impacts)) \n",
    "        upper_bound.append(np.percentile(impacts, 97.5))\n",
    "        \n",
    "    plot_range = list(var_to_impacts.keys())\n",
    "    plt.plot(plot_range, median, label=nudge_type)\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(\"95%\"), alpha=0.2)\n",
    "\n",
    "    \n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"intervention impact\")\n",
    "plt.title(\"impact of random interventions for {}% of the max entropy\".format(PERCENTAGE_MAX_ENTROPY_SIZE))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data experiment impact on MI of minimalizing individual nudge impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_files_json(file_names):\n",
    "    dicts = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as f:\n",
    "            dicts.append(json.load(f))\n",
    "            \n",
    "    return dicts\n",
    "\n",
    "def update_dictkeys_unicode2string(dictionary):\n",
    "    return {str(k):v for k,v in dictionary.items()}\n",
    "\n",
    "def combine_data_dicts(dictionaries):\n",
    "    \"\"\"\n",
    "    Combine data dicts, meaning that all data of the same key are merged into a new dict\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionaries: a list of dicts\n",
    "        Every key of the dicts should have a list as values \n",
    "\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for dictionary in dictionaries:\n",
    "        for key, values in dictionary.items():\n",
    "            if key in new_dict:\n",
    "                new_dict[key].extend(values)\n",
    "            else:\n",
    "                new_dict[key] = values\n",
    "                \n",
    "    return new_dict\n",
    "\n",
    "def get_average_values_dict(dictionary):\n",
    "    \"\"\"take average of all values dictionary\"\"\"\n",
    "    return {k:np.mean(v) for k, v in dictionary.items()}\n",
    "\n",
    "#for 2 input vars with 5 states\n",
    "file_name_format2 = \"minimize_individual_focused_nudge_impact_kl_divergence_inspect_change_MI_2var_5states{}.json\"\n",
    "files_vars2 = [PATH + file_name_format2.format(i) for i in [\"\", 1, 3, 4, 5, 6]]\n",
    "dictionaries_vars2 = load_files_json(files_vars2)\n",
    "dictionaries_vars2 = [update_dictkeys_unicode2string(dictionary) for dictionary in dictionaries_vars2]\n",
    "dictionary_vars2 = combine_data_dicts(dictionaries_vars2)\n",
    "print(get_average_values_dict(dictionary_vars2))\n",
    "\n",
    "#for 3 input vars with 5 states\n",
    "file_name_format3 = \"minimize_individual_focused_nudge_impact_inspect_change_MI_3var_5states{}.json\"\n",
    "files_vars3 = [PATH + file_name_format3.format(i) for i in [\"\", 2, 3, 4, 5]]\n",
    "dictionaries_vars3 = load_files_json(files_vars3)\n",
    "dictionaries_vars3 = [update_dictkeys_unicode2string(dictionary) for dictionary in dictionaries_vars3]\n",
    "dictionary_vars3 = combine_data_dicts(dictionaries_vars3)\n",
    "print(get_average_values_dict(dictionary_vars3))\n",
    "\n",
    "#something is wrong !!! mutual information 4 variables waay to high\n",
    "file_vars4 = \"minimize_individual_focused_nudge_impact_inspect_change_MI_4var_5states.json\"\n",
    "dictionaries_vars4 = load_files_json([PATH + file_vars4])\n",
    "dictionary_vars4 = update_dictkeys_unicode2string(dictionaries_vars4[0])\n",
    "print(get_average_values_dict(dictionary_vars4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation MI and individual nudge impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_correlation_MI_nudge_impact(var_to_file_names, multiple_files4=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    var_to_files: a dict\n",
    "    \n",
    "    \"\"\"\n",
    "    var_to_MI_and_nudge_impact = {}\n",
    "    for number_of_var, file_names in var_to_file_names.items():\n",
    "        file_names = [PATH+file_name for file_name in file_names]\n",
    "        if number_of_var == 5 or (number_of_var == 4 and multiple_files4):\n",
    "            mi_and_nudge_impact_lists = load_files_json(file_names)\n",
    "            var_to_MI_and_nudge_impact[number_of_var] = [\n",
    "                list(itertools.chain.from_iterable(mi_and_nudge_impact_lists))\n",
    "            ]\n",
    "        else:\n",
    "            var_to_MI_and_nudge_impact[number_of_var] = load_files_json(file_names)[0]\n",
    "\n",
    "    return var_to_MI_and_nudge_impact\n",
    "\n",
    "focused_nudge_kl_divergence_impact_and_MI_var_to_files = {\n",
    "    1: [\"correlation_MI_individual_nudge_impact_1var_5states_min_output001_ex1.json\"],\n",
    "    2: [\"correlation_MI_individual_nudge_impact_2var_5states_min_output001_ex2.json\"],\n",
    "    3: [\"correlation_MI_individual_nudge_impact_3var_5states_min_output001_exp3.json\"],\n",
    "    4: [\"correlation_MI_individual_nudge_impact_4var_5states_min_output001_exp4.json\"],\n",
    "    5: [\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_exp5_points0_20.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_exp6_point20_40.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_exp3_points40_60.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_ex2_points60_80.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_ex1_points80_100.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "focused_nudge_kl_divergence_impact_and_MI_var_to_files2 = {\n",
    "    1: [\"correlation_MI_individual_nudge_impact_1var_5states_min_output001_ex1_0_300.json\"],\n",
    "    2: [\"correlation_MI_individual_nudge_impact_2var_5states_min_output001_ex3_0_300.json\"],\n",
    "    3: [\"correlation_MI_individual_nudge_impact_3var_5states_min_output001_ex1_0_300.json\"],\n",
    "    4: [\n",
    "        \"correlation_MI_individual_nudge_impact_4var_5states_min_output001_exp4.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_4var_5states_min_output001_ex4_200_250.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_4var_5states_min_output001_ex5_250_300.json\"\n",
    "    ],\n",
    "    5: [\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_exp5_points0_20.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_exp6_point20_40.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_exp3_points40_60.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_ex2_points60_80.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_min_output001_ex1_points80_100.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "focused_nudge_l1norm_impact_and_MI_var_to_files = {\n",
    "    1: [\"correlation_MI_individual_nudge_impact_1var_5states_l1norm_ex1.json\"],\n",
    "    2: [\"correlation_MI_individual_nudge_impact_2var_5states_l1norm_ex2.json\"],\n",
    "    3: [\"correlation_MI_individual_nudge_impact_3var_5states_l1norm_exp3.json\"],\n",
    "    4: [\n",
    "        \"correlation_MI_individual_nudge_impact_4var_5states_l1norm_exp4.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_4var_5states_l1norm_exp1_200_250.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_4var_5states_l1norm_exp2_250_300.json\"\n",
    "    ],\n",
    "    5: [\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_l1norm_exp5_points0_20.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_l1norm_exp6_point20_40.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_l1norm_ex1_points40_60.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_l1norm_ex2_points60_80.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_l1norm_ex1_points80_90.json\",\n",
    "        \"correlation_MI_individual_nudge_impact_5var_5states_l1norm_ex2_points90_100.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "vector_nudge_kl_divergence_impact_and_MI_var_to_files = {\n",
    "    1: [\"correlation_MI_individual_vector_nudge_impact_1var_5states_min_output001_ex1.json\"],\n",
    "    2: [\"correlation_MI_individual_vector_nudge_impact_2var_5states_kl-divergence_ex2.json\"],\n",
    "    3: [\"correlation_MI_individual_vector_nudge_impact_3var_5states_kl-divergence_ex1.json\"],\n",
    "    4: [\"correlation_MI_individual_vector_nudge_impact_4var_5states_kl-divergence_ex2.json\"],\n",
    "    5: [\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_kl-divergence_ex3_points0_30.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_kl-divergence_ex1_points30_60.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_kl-divergence_ex1_points60_70.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_kl-divergence_ex2_points70_80.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_kl-divergence_ex3_points80_90.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_kl-divergence_ex4_points90_100.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "vector_nudge_l1norm_impact_and_MI_var_to_files = {\n",
    "    1: [\"correlation_MI_individual_vector_nudge_impact_1var_5states_l1norm_ex1.json\"],\n",
    "    2: [\"correlation_MI_individual_vector_nudge_impact_2var_5states_l1norm_ex2.json\"],\n",
    "    3: [\"correlation_MI_individual_vector_nudge_impact_3var_5states_l1norm_ex3.json\"],\n",
    "    4: [\"correlation_MI_individual_vector_nudge_impact_4var_5states_l1norm_ex4.json\"],\n",
    "    5: [\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_l1norm_ex5_point0_30.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_l1norm_exp6_points30_60.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_l1norm_ex5_point60_90.json\",\n",
    "        \"correlation_MI_individual_vector_nudge_impact_5var_5states_l1norm_exp6_points90_120.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "var_to_focused_nudge_kl_divergence_impact_and_MI = load_data_correlation_MI_nudge_impact(\n",
    "    focused_nudge_kl_divergence_impact_and_MI_var_to_files2, True\n",
    ")\n",
    "\n",
    "var_to_focused_nudge_l1norm_impact_and_MI = load_data_correlation_MI_nudge_impact(\n",
    "    focused_nudge_l1norm_impact_and_MI_var_to_files, True\n",
    ")\n",
    "\n",
    "var_to_vector_nudge_kl_divergence_impact_and_MI = load_data_correlation_MI_nudge_impact(\n",
    "    vector_nudge_kl_divergence_impact_and_MI_var_to_files\n",
    ")\n",
    "\n",
    "var_to_vector_nudge_l1norm_impact_and_MI = load_data_correlation_MI_nudge_impact(\n",
    "    vector_nudge_l1norm_impact_and_MI_var_to_files\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#add seaborn linregress on own computer\n",
    "\n",
    "# dict_to_use = var_to_focused_nudge_kl_divergence_impact_and_MI\n",
    "# dict_to_use = var_to_focused_nudge_l1norm_impact_and_MI\n",
    "dict_to_use = var_to_vector_nudge_kl_divergence_impact_and_MI\n",
    "# dict_to_use = var_to_vector_nudge_l1norm_impact_and_MI\n",
    "\n",
    "for var, impact_nudges_and_mi in dict_to_use.items():\n",
    "    if var == 5:\n",
    "#     if var == 5 or var ==4:\n",
    "        impact_nudges_and_mi=impact_nudges_and_mi[0]\n",
    "\n",
    "    impact_nudges = [item[0] for item in impact_nudges_and_mi] \n",
    "    mutual_information_sizes = [item[1] for item in impact_nudges_and_mi]\n",
    "        \n",
    "#   testing for normality \n",
    "    print(stats.shapiro(impact_nudges))\n",
    "    print(stats.normaltest(impact_nudges))\n",
    "    stats.probplot(impact_nudges, dist=\"norm\", plot=plt)\n",
    "    plt.show()\n",
    "    sns.distplot(impact_nudges)\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    plt.plot(mutual_information_sizes, impact_nudges, 'o')\n",
    "    plt.xlabel(\"mutual information\")\n",
    "    plt.ylabel(\"nudge impact\")\n",
    "    plt.show()\n",
    "    print(stats.linregress(impact_nudges, mutual_information_sizes))\n",
    "    print(\"spearman\")\n",
    "    print(stats.spearmanr(impact_nudges, mutual_information_sizes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# dict_to_use = var_to_focused_nudge_kl_divergence_impact_and_MI\n",
    "dict_to_use = var_to_focused_nudge_l1norm_impact_and_MI\n",
    "# dict_to_use = var_to_vector_nudge_kl_divergence_impact_and_MI\n",
    "# dict_to_use = var_to_vector_nudge_l1norm_impact_and_MI\n",
    "\n",
    "for var, impact_nudges_and_mi in dict_to_use.items():\n",
    "    #if var == 5:\n",
    "    if var == 5 or var ==4:\n",
    "        impact_nudges_and_mi = impact_nudges_and_mi[0]       \n",
    "        impact_nudges = [item[0] for item in impact_nudges_and_mi]\n",
    "        print(len(impact_nudges))\n",
    "        mutual_information_sizes = [item[1] for item in impact_nudges_and_mi]\n",
    "    else:\n",
    "        impact_nudges = [item[0] for item in impact_nudges_and_mi] \n",
    "        print(len(impact_nudges))\n",
    "        mutual_information_sizes = [item[1] for item in impact_nudges_and_mi]\n",
    "\n",
    "    \n",
    "#     df = pd.DataFrame({\"mutual information\": mutual_information_sizes, \"l1-norm impact\": impact_nudges}) \n",
    "#     sns.regplot(\"mutual information\", \"l1-norm impact\", df)\n",
    "    df = pd.DataFrame({\"mutual information\": mutual_information_sizes, \"KL-divergence impact\": impact_nudges}) \n",
    "    sns.regplot(\"mutual information\", \"KL-divergence impact\", df)\n",
    "    plt.title(\"correlation mutual information and causal power\")\n",
    "    sns.plt.show()\n",
    "    print(stats.linregress(impact_nudges, mutual_information_sizes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#add seaborn linregress on own computer\n",
    "\n",
    "focused_nudges = var_to_focused_nudge_kl_divergence_impact_and_MI\n",
    "focused_nudges[1] = [i for i in focused_nudges[1] if i[0] < 0.00025]\n",
    "\n",
    "vector_nudges = var_to_vector_nudge_kl_divergence_impact_and_MI\n",
    "vector_nudges[1] = [i for i in vector_nudges[1] if i[0] < 0.00006]\n",
    "\n",
    "for var, impact_nudges_and_mi in focused_nudges.items():\n",
    "    if var == 5:\n",
    "        impact_nudges_and_mi=impact_nudges_and_mi[0]\n",
    "\n",
    "    impact_nudges = [item[0] for item in impact_nudges_and_mi] \n",
    "    mutual_information_sizes = [item[1] for item in impact_nudges_and_mi]\n",
    "        \n",
    "    #   testing for normality \n",
    "    print(stats.shapiro(impact_nudges))\n",
    "    print(stats.normaltest(impact_nudges))\n",
    "    sns.distplot(impact_nudges)\n",
    "    plt.show()\n",
    "        \n",
    "    plt.plot(mutual_information_sizes, impact_nudges, 'o')\n",
    "    plt.xlabel(\"mutual information\")\n",
    "    plt.ylabel(\"nudge impact\")\n",
    "    plt.show()\n",
    "    print(stats.linregress(impact_nudges, mutual_information_sizes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation MI and nudge impact for 1 input and 1 output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def plot_nudge_impact_given_mi(data_dict, title, ylabel, yrange=None):\n",
    "    plot_range, mean, std, batches_std = find_mean_std_mse(data_dict, batch_size=1)\n",
    "    lower_bound = np.array(mean) - 2*np.array(std)\n",
    "    upper_bound = np.array(mean) + 2*np.array(std)\n",
    "    plt.plot(plot_range, mean, label=\"mean\")\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(\"std\"),\n",
    "                     alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"mutual information\")\n",
    "    plt.ylabel(ylabel)\n",
    "    #plt.legend()\n",
    "    plt.title(title)\n",
    "    if yrange:\n",
    "        plt.ylim(yrange)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print(stats.linregress(np.array(plot_range), np.array(mean)))\n",
    "\n",
    "def string_keys_to_numbers(data_dict):\n",
    "    for mi_value, values in data_dict.items():\n",
    "        data_dict[round((float(mi_value)*100.0))/100.0] = values\n",
    "        data_dict.pop(mi_value, None)\n",
    "    \n",
    "    return data_dict\n",
    "    \n",
    "#load the data\n",
    "with open(\"data_experiments/individual_nudge_impact_kl_divergence_focused_given_MI2.json\", 'r') as f:\n",
    "    focused_kl_divergence = json.load(f)\n",
    "    \n",
    "with open(\"data_experiments/individual_nudge_impact_l1norm_focused_given_MI2.json\", 'r') as f:\n",
    "    focused_l1norm = json.load(f)\n",
    "\n",
    "with open(\"data_experiments/individual_nudge_impact_kl_divergence_vector_given_MI2.json\", 'r') as f:\n",
    "    vector_kl_divergence = json.load(f)\n",
    "\n",
    "with open(\"data_experiments/individual_nudge_impact_absolute_vector_given_MI2.json\", 'r') as f:\n",
    "    vector_l1norm = json.load(f)\n",
    "    \n",
    "focused_kl_divergence = string_keys_to_numbers(focused_kl_divergence)\n",
    "focused_l1norm = string_keys_to_numbers(focused_l1norm)\n",
    "vector_kl_divergence = string_keys_to_numbers(vector_kl_divergence)\n",
    "vector_l1norm = string_keys_to_numbers(vector_l1norm)\n",
    "\n",
    "def plot_median_95_percent_confidence_interval(vars_to_values, yrange=None, xlabel=None, ylabel=None,\n",
    "                                               title=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    values_dict: a dict where every variable has a list as values \n",
    "    \n",
    "    \"\"\"\n",
    "    vars_to_values = OrderedDict(sorted(vars_to_values.items(), key=lambda x: x[0]))\n",
    "    #add confidence intervals based on percentiles\n",
    "    lower_bound, median, mean, upper_bound = [], [], [], []\n",
    "    for mutual_information, impacts in vars_to_values.items():\n",
    "        lower_bound.append(np.percentile(impacts, 2.5))\n",
    "        median.append(np.median(impacts)) \n",
    "        mean.append(np.mean(impacts))\n",
    "        upper_bound.append(np.percentile(impacts, 97.5))\n",
    "\n",
    "    plot_range = sorted(list(focused_kl_divergence.keys()))\n",
    "    print(title)\n",
    "    print(stats.linregress(mean, plot_range))\n",
    "    print(stats.spearmanr(mean, plot_range))\n",
    "    \n",
    "    plt.plot(plot_range, median, label=\"median\")\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(\"95%\"), alpha=0.2)\n",
    "    if yrange:\n",
    "        plt.ylim(yrange)\n",
    "        \n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_median_95_percent_confidence_interval(\n",
    "    focused_kl_divergence, (0, 0.0002), xlabel=\"mutual information\", ylabel=\"KL-divergence\",\n",
    "    title=\"relation mutual information and focused intervention impact\" \n",
    ")\n",
    "plot_median_95_percent_confidence_interval(\n",
    "    focused_l1norm, xlabel=\"mutual information\", ylabel=\"l1-norm\",\n",
    "    title=\"relation mutual information and focused intervention impact\"\n",
    ")\n",
    "plot_median_95_percent_confidence_interval(\n",
    "    vector_kl_divergence, (0, 0.000125), xlabel=\"mutual information\", ylabel=\"KL-divergence\",\n",
    "    title=\"relation mutual information and individual intervention impact\"\n",
    ")\n",
    "plot_median_95_percent_confidence_interval(\n",
    "    vector_l1norm, xlabel=\"mutual information\", ylabel=\"l1-norm\",\n",
    "    title=\"relation mutual information and individual intervention impact\"\n",
    ")\n",
    "\n",
    "#confidence intervals based on mean and standard deviations\n",
    "# plot_nudge_impact_given_mi(\n",
    "#     focused_kl_divergence, \n",
    "#     \"impact of focused interventions for a fixed mutual information\", \n",
    "#     \"KL-divergence impact\",\n",
    "#     (-0.00005, 0.0002)\n",
    "# )\n",
    "\n",
    "# plot_nudge_impact_given_mi(\n",
    "#     focused_l1norm, \"impact of focused interventions for a fixed mutual information\", \"l1-norm impact\"\n",
    "# )\n",
    "# plot_nudge_impact_given_mi(\n",
    "#     vector_kl_divergence, \n",
    "#     \"impact of individual interventions for a fixed mutual information\", \n",
    "#     \"KL-divergence impact\",\n",
    "#     (-0.00004, 0.00012)\n",
    "# )\n",
    "# plot_nudge_impact_given_mi(\n",
    "#     vector_l1norm, \"impact of individual interventions for a fixed mutual information\", \"l1-norm impact\"\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data_experiments/individual_nudge_impact_kl_divergence_focused_given_MI2.json\", 'r') as f:\n",
    "    focused_kl_divergence = json.load(f)\n",
    "    \n",
    "with open(\"data_experiments/individual_nudge_impact_kl_divergence_vector_given_MI2.json\", 'r') as f:\n",
    "    vector_kl_divergence = json.load(f)\n",
    "    \n",
    "focused_kl_divergence = string_keys_to_numbers(focused_kl_divergence)\n",
    "vector_kl_divergence = string_keys_to_numbers(vector_kl_divergence)\n",
    "\n",
    "print(np.array(focused_kl_divergence[0.85]))\n",
    "print(np.array(vector_kl_divergence[0.85]))\n",
    "focused_kl_divergence[0.85] = [i for i in focused_kl_divergence[0.85] if i < 1e-03]\n",
    "vector_kl_divergence[0.85] = [i for i in vector_kl_divergence[0.85] if i < 1e-04]\n",
    "\n",
    "plot_nudge_impact_given_mi(\n",
    "    focused_kl_divergence, \"focused nudge impact for a fixed mutual information cleaned data\", \"nudge impact (KL-divergence)\"\n",
    ")\n",
    "plot_nudge_impact_given_mi(\n",
    "    vector_kl_divergence, \"vector nudge impact for a fixed mutual information cleaned data\", \"nudge impact (KL-divergence)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data_experiments/relation_MI_nudge_impact_1input_non_biased1.json\", 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "with open(\"data_experiments/relation_MI_nudge_impact_1input_non_biased2.json\", 'r') as f:\n",
    "    data_dict1 = json.load(f)\n",
    "    \n",
    "for mi_value, values in data_dict1.items():\n",
    "    data_dict1[round((float(mi_value)*1000.0))/1000.0] = values\n",
    "    data_dict1.pop(mi_value, None)\n",
    "    \n",
    "for mi_value, values in data_dict.items():\n",
    "    data_dict[round((float(mi_value)*100.0))/100.0] = values\n",
    "    data_dict.pop(mi_value, None)\n",
    "    \n",
    "data_dict.update(data_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print({k:len(v) for k,v in data_dict.items()})\n",
    "plot_range, mean, std, batches_std = find_mean_std_mse(data_dict, batch_size=1)\n",
    "print(np.array(data_dict[0.85]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_bound = np.array(mean)-np.array(std)\n",
    "upper_bound = np.array(mean)+np.array(std)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(plot_range)\n",
    "plt.plot(plot_range, mean, label=\"mean\")\n",
    "plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"std\"),\n",
    "                 alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"mutual information\")\n",
    "plt.ylabel(\"KL-divergence\")\n",
    "#plt.legend()\n",
    "plt.title('Individual nudge impact for set mutual information')\n",
    "plt.show()\n",
    "\n",
    "stats.linregress(np.array(plot_range), np.array(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict[0.85] = [i for i in data_dict[0.85] if i < 1e-03]\n",
    "print(np.array(data_dict[0.85]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_range, mean, std, batches_std = find_mean_std_mse(data_dict, batch_size=1)\n",
    "lower_bound = np.array(mean)-np.array(std)\n",
    "upper_bound = np.array(mean)+np.array(std)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(plot_range)\n",
    "plt.plot(plot_range, mean, label=\"mean\")\n",
    "plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"std\"),\n",
    "                 alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"mutual information\")\n",
    "plt.ylabel(\"KL-divergence\")\n",
    "#plt.legend()\n",
    "plt.title('Individual nudge impact for set mutual information')\n",
    "plt.show()\n",
    "\n",
    "stats.linregress(np.array(plot_range), np.array(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Make correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confidence_interval_r2(correlation, N, confidence_interval):\n",
    "    z = np.arctanh(correlation)\n",
    "    sigma = (1/((N-3)**0.5))\n",
    "    #print(sigma)\n",
    "\n",
    "    cint = z + np.array([-1, 1]) * sigma * stats.norm.ppf((1+confidence_interval)/2)\n",
    "    return np.tanh(cint)\n",
    "\n",
    "correlation = 0.711\n",
    "N = 300\n",
    "confidence_interval = 0.95\n",
    "confidence_interval_r2(correlation, N, confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def produce_lower_upper_bound(correlations, sample_numbers):\n",
    "    confidence_intervals = [confidence_interval_r2(correlation, samples, 0.95) for correlation, samples \n",
    "                            in zip(correlations, sample_numbers)]\n",
    "    confidence_intervals = [list(i) for i in confidence_intervals]\n",
    "    lower_bound, upper_bound = zip(*confidence_intervals)\n",
    "    return lower_bound, upper_bound\n",
    "    \n",
    "print(produce_lower_upper_bound(focused_l1norm_correlations, [300, 300, 300, 300, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_vars = [1, 2, 3, 4, 5]\n",
    "focused_l1norm_correlations = [0.711, 0.5359, 0.5378, 0.5200, 0.5209]\n",
    "focused_kl_divergence_correlations = [0.2810, 0.4102, 0.5124, 0.5014, 0.5729]\n",
    "vector_l1norm_correlations = [0.762, 0.7241, 0.6156, 0.576, 0.4808]\n",
    "vector_kl_divergence_correlations = [0.2424, 0.6589, 0.6628, 0.5299, 0.5707]\n",
    "\n",
    "\n",
    "plt.plot(input_vars, focused_l1norm_correlations)\n",
    "lower_bound, upper_bound = produce_lower_upper_bound(focused_l1norm_correlations, [300, 300, 300, 300, 100])\n",
    "plt.fill_between(input_vars, lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"95%\"),\n",
    "                 alpha=0.2)\n",
    "plt.xlabel(\"number of input variables\")\n",
    "plt.ylabel(\"correlation MI and causal power\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(input_vars, focused_kl_divergence_correlations)\n",
    "lower_bound, upper_bound = produce_lower_upper_bound(focused_kl_divergence_correlations, [300, 300, 300, 300, 100])\n",
    "plt.fill_between(input_vars, lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"95%\"),\n",
    "                 alpha=0.2)\n",
    "plt.xlabel(\"number of input variables\")\n",
    "plt.ylabel(\"correlation MI and causal power\")\n",
    "plt.show()\n",
    "\n",
    "#produce the plot for vector interventions measured with the l1-norm\n",
    "plt.plot(input_vars, vector_l1norm_correlations)\n",
    "lower_bound, upper_bound = produce_lower_upper_bound(vector_l1norm_correlations, [300, 300, 300, 300, 100])\n",
    "plt.fill_between(input_vars, lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"95%\"),\n",
    "                 alpha=0.2)\n",
    "plt.xlabel(\"number of input variables\")\n",
    "plt.ylabel(\"correlation MI and causal power\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(input_vars, vector_kl_divergence_correlations)\n",
    "lower_bound, upper_bound = produce_lower_upper_bound(vector_kl_divergence_correlations, [300, 300, 300, 300, 100])\n",
    "plt.fill_between(input_vars, lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"95%\"),\n",
    "                 alpha=0.2)\n",
    "plt.xlabel(\"number of input variables\")\n",
    "plt.ylabel(\"correlation MI and causal power\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show output marginal has less states with low probability as number of variables increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "#based on 1000 distributions\n",
    "def dict_keys_from_string_to_int(dictionary):\n",
    "    return {int(k):v for k,v in dictionary.items()}\n",
    "\n",
    "\n",
    "filename =\"/home/derkjan/Documents/academics_UVA/master_thesis/code/data_experiments/states_below_0.1.json\"\n",
    "with open(filename, 'r') as f:\n",
    "    number_of_vars_to_low_probable_states = json.load(f)\n",
    "    \n",
    "number_of_vars_to_low_probable_states = dict_keys_from_string_to_int(number_of_vars_to_low_probable_states)\n",
    "print(number_of_vars_to_low_probable_states)\n",
    "\n",
    "plt.plot(number_of_vars_to_low_probable_states.keys(), number_of_vars_to_low_probable_states.values())\n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"number of output marginals below 0.1\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename =\"/home/derkjan/Documents/academics_UVA/master_thesis/code/data_experiments/distances_output_uniform.json\"\n",
    "with open(filename, 'r') as f:\n",
    "    number_of_vars_to_distance_output_uniform = json.load(f)\n",
    "    \n",
    "variable_range, mean, std, batches_std = (\n",
    "    find_mean_std_mse(number_of_vars_to_distance_output_uniform, 30)\n",
    ")\n",
    "print(mean)\n",
    "print(std)\n",
    "lower_bound = np.array(mean) - 2*np.array(batches_std)\n",
    "upper_bound = np.array(mean) + 2*np.array(batches_std)\n",
    "\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "\n",
    "plt.plot(range(1,6,1), mean)\n",
    "plt.fill_between(range(1,6,1), lower_bound, upper_bound, \n",
    "                 label='{}'.format(\"95%\"),\n",
    "                 alpha=0.2)\n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"distance to uniform distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
