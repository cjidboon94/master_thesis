{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import json\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "import powerlaw\n",
    "from jointpdf.jointpdf import JointProbabilityMatrix\n",
    "from jointpdf.jointpdf import FullNestedArrayOfProbabilities\n",
    "\n",
    "from extension_probability_matrix import JointProbabilityMatrixExtended\n",
    "import probability_distributions\n",
    "from probability_distributions import ProbabilityArray\n",
    "from simulate import find_mean_std_mse\n",
    "import nudge\n",
    "import plotting\n",
    "\n",
    "import information_theory\n",
    "from information_theory import calculate_mutual_information\n",
    "\n",
    "import maximum_nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distribution(shape, method, arguments=None):\n",
    "    if method=='random_biased':\n",
    "        distribution = np.random.random(shape)\n",
    "        distribution = distribution/np.sum(distribution)\n",
    "        return distribution\n",
    "    elif method=='random_dirichlet':\n",
    "        return probability_distributions.compute_joint_uniform_random(shape)\n",
    "    elif method=='fixed_entropy':\n",
    "        return probability_distributions.generate_probability_distribution_with_certain_entropy(\n",
    "            shape, arguments['entropy_size']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('provide a valid method')\n",
    "        \n",
    "def calculate_amount_and_size_nudges(total_nudge_size, number_of_states, threshold=10):\n",
    "    \"\"\"\n",
    "    Calculate the nudge size and the number of nudges that need to be performed \n",
    "    to nudge a variable with the total nudge size. Assuming the distribution is\n",
    "    not too peaked, in other words, not too many states should have a probability\n",
    "    that is 10 times smaller than normal.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    total_nudge_size: a number\n",
    "        How much the variable need to be nudged\n",
    "    number_of_states: a number\n",
    "        The total number of states of the joint distribution\n",
    "    threshold: a float \n",
    "        Indicating how much smaller than uniform the value of the number\n",
    "        at the 95-99 percentile of points is. Defaults to 10 \n",
    "        \n",
    "    Returns: local_nudge, number_of_nudges\n",
    "    -------\n",
    "    local_nudge: a number \n",
    "        The size of the local nudge to be performed on the joint distribution\n",
    "    number_of_nudges: integer\n",
    "        How often the nudge need to be performed\n",
    "    \n",
    "    \"\"\"\n",
    "    assumed_min_size = 1.0/threshold\n",
    "    max_local_nudge = min(total_nudge_size, 0.1/number_of_states)\n",
    "    number_of_nudges = int(np.ceil(total_nudge_size/max_local_nudge))\n",
    "    local_nudge = total_nudge_size/float(number_of_nudges) \n",
    "    return local_nudge, number_of_nudges\n",
    "\n",
    "def percentage_max_entropy(shape, percentage):\n",
    "    \"\"\" \n",
    "    Return the percentage of the max-entropy given the shape\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    shape: iterable\n",
    "    percentage: float\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.log2(reduce(lambda x,y: x*y, shape)) * percentage\n",
    "\n",
    "def percentage_states_max_entropy(shape, percentage):\n",
    "    \"\"\" \n",
    "    Return the percentage of the max-entropy given the shape\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    shape: iterable\n",
    "    percentage: float\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.log2(reduce(lambda x,y: x*y, shape) * percentage)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make one cell for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mean_and_confidence(plot_range, mean, mean_label, confidence_interval, \n",
    "                             confidence_interval_title):\n",
    "    \"\"\"\n",
    "    Plot the mean and some kind of confidence interval (standard deviation or\n",
    "    mean-squared-error)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    plot_range: iterable\n",
    "    mean: an iterable\n",
    "        the mean of the values at that point\n",
    "    confidence_interval: an iterable\n",
    "        Representing the  interval of confidence in that point. \n",
    "        The iterable should have length plot_range.\n",
    "    confidence_interval_title: a string\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lower_bound = np.array(mean)-np.array(confidence_interval)\n",
    "    upper_bound = np.array(mean)+np.array(confidence_interval)\n",
    "    plt.plot(plot_range, mean, label=mean_label)\n",
    "    plt.fill_between(plot_range, lower_bound, upper_bound, \n",
    "                     label='{}'.format(confidence_interval_title),\n",
    "                     alpha=0.2)\n",
    "    \n",
    "def plot_results(*args, **kwargs):\n",
    "    \"\"\"plot results from simulations\n",
    "    \n",
    "    Parameters:\n",
    "        args: 1 or more dicts. The dicts should have for the keys numerical\n",
    "            input values and for the values iterables of numbers.\n",
    "        kwargs: at least the arguments xlabel, ylabel, title\n",
    "        \n",
    "    \"\"\"\n",
    "    for argument in args:\n",
    "        data, meta_dict = argument\n",
    "        variable_range, mean, std, batches_std = (\n",
    "            find_mean_std_mse(data, 10)\n",
    "        )\n",
    "        \n",
    "        if kwargs['std_of_batches']: \n",
    "            plot_mean_and_confidence(variable_range, mean, meta_dict['mean_label'], \n",
    "                                     batches_std, \"batches stdev\")\n",
    "        else:\n",
    "            plot_mean_and_confidence(variable_range, mean, meta_dict['mean_label'], \n",
    "                                     std, \"stdev\")\n",
    "    \n",
    "    plt.xlabel(kwargs['xlabel'])\n",
    "    plt.ylabel(kwargs['ylabel'])\n",
    "    plt.legend()\n",
    "    plt.title(kwargs['title'])\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "Check the distance between a Dirichlet distribution and the uniform distribution for an \n",
    "increasing number of states.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000\n",
    "min_number_of_states = 2\n",
    "max_number_of_states = 250\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    entropies_dirichlet = []\n",
    "    for i in range(min_number_of_states, max_number_of_states, 1):\n",
    "        distances = []\n",
    "        for _ in range(SAMPLE_SIZE):\n",
    "            dirichlet_dist = np.random.dirichlet([1]*i)\n",
    "            #print(dirichlet_dist)\n",
    "            distance = entropy(dirichlet_dist, base=2)\n",
    "            distances.append(distance)\n",
    "\n",
    "        entropies_dirichlet.append(np.mean(distances))\n",
    "\n",
    "    print(entropies_dirichlet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    plt.plot(list(range(2,250,1)), entropies_dirichlet, label=\"entropy Dirichlet\")\n",
    "    plt.plot(list(range(2,250,1)), np.log2(list(range(2,250,1))), label=\"max entropy\")\n",
    "    plt.xlabel(\"number of states\")\n",
    "    plt.ylabel(\"entropy\")\n",
    "    plt.title('entropy Dirichlet weights close to maximum')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2A\n",
    "\n",
    "See what the distance is between 2 randomly generated functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the average \"distance\" (KL-divergence) between randomly (probability masses states are distributed according to Dirichlet distribution) generated distributions. The distance decreases, since, the number of\n",
    "states that are close to uniform increases as the number of states for a distribution grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states, number_of_variables = 5, 4\n",
    "shape = tuple([number_of_states]*(number_of_variables+1))\n",
    "total_nudge_size = 0.01\n",
    "total_number_of_states = number_of_states**number_of_variables\n",
    "max_local_nudge, number_of_nudges = calculate_amount_and_size_nudges(\n",
    "    total_nudge_size, total_number_of_states\n",
    ")\n",
    "RUN = False\n",
    "if RUN:\n",
    "    total_nudge_sizes = []\n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        #distribution = ProbabilityArray(generate_distribution(shape, 'random_dirichlet'))\n",
    "        distribution = ProbabilityArray(generate_distribution(\n",
    "            shape, 'fixed_entropy', \n",
    "            {\"entropy_size\":percentage_states_max_entropy(shape, 0.1)}\n",
    "        ))\n",
    "        function_labels, label_nudged_variable = set([number_of_variables]), 0\n",
    "        input_variable_labels = set(range(len(distribution.probability_distribution.shape))) - function_labels\n",
    "        input_distribution = distribution.marginalize(input_variable_labels)\n",
    "        marginal_nudged_old = ProbabilityArray(input_distribution).marginalize(\n",
    "            set([label_nudged_variable])\n",
    "        ) \n",
    "\n",
    "        new_input_distribution = nudge.nudge_distribution_local_non_causal(\n",
    "            input_distribution, 0, max_local_nudge, number_of_nudges\n",
    "        )\n",
    "        marginal_nudged_new = ProbabilityArray(new_input_distribution).marginalize(\n",
    "            set([label_nudged_variable])\n",
    "        ) \n",
    "        total_nudge_sizes.append(np.sum(np.absolute(marginal_nudged_old-marginal_nudged_new)))\n",
    "\n",
    "    print(np.mean(total_nudge_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states, number_of_distributions = 5, 20\n",
    "difference_distributions = []\n",
    "RUN = False\n",
    "if RUN:\n",
    "    for number_of_variables in range(1, 6, 1):\n",
    "        marginal_outputs = []\n",
    "        shape = tuple([number_of_states]*(number_of_variables+1))\n",
    "        for i in range(number_of_distributions):\n",
    "            print(i)\n",
    "            #distribution = ProbabilityArray(generate_distribution(shape, 'random_dirichlet'))\n",
    "            distribution = ProbabilityArray(generate_distribution(\n",
    "                shape, 'fixed_entropy', \n",
    "                {\"entropy_size\":percentage_max_entropy(shape, 0.75)}\n",
    "            ))\n",
    "            function_label, label_nudged_variable = number_of_variables, 0\n",
    "            marginal_outputs.append(distribution.marginalize(set([function_label])))\n",
    "\n",
    "        kl_divergences = []\n",
    "        for i in range(int(number_of_distributions/2.0)):\n",
    "            kl_divergences.append(entropy(marginal_outputs[i].flatten(), marginal_outputs[i+1].flatten()))\n",
    "\n",
    "        difference_distributions.append(np.mean(kl_divergences))\n",
    "        print(np.mean(kl_divergences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2B \n",
    "\n",
    "The actual experiment- for non-causal nudges. Both for local and not local nudges. Local nudges can be performed with vector and focused nudges, while non-local nudges are always vector nudges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "    \n",
    "def product(it):\n",
    "    \"\"\"calculate the product of all terms in the iterable\"\"\"\n",
    "    return reduce(lambda x, y: x*y, it)\n",
    "    \n",
    "def calculate_focused_nudge_impact(distribution, total_nudge_size):\n",
    "    \"\"\" \n",
    "    For now calculate the impact of a local non-causal nudge on the input variables\n",
    "    on the completely causally determined output variable\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    distribution: a ProbabilityArray object\n",
    "    total_nudge_size: number\n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_variables = len(distribution.probability_distribution.shape)\n",
    "    total_number_of_states = product(distribution.probability_distribution.shape)\n",
    "    max_local_nudge, number_of_nudges = calculate_amount_and_size_nudges(\n",
    "        total_nudge_size, total_number_of_states\n",
    "    )\n",
    "    function_labels, label_nudged_variable = set([number_of_variables-1]), 0\n",
    "    input_variable_labels = set(range(number_of_variables)) - function_labels\n",
    "    input_distribution = distribution.marginalize(input_variable_labels)\n",
    "    new_input_distribution = nudge.nudge_distribution_local_non_causal(\n",
    "        input_distribution, 0, max_local_nudge, number_of_nudges\n",
    "    )\n",
    "    return nudge.impact_nudge_causal_output(distribution, function_labels,\n",
    "                                            new_input_distribution)\n",
    "\n",
    "def calculate_non_local_nudge_impact(distribution, total_nudge_size, nudged_variables):\n",
    "    \"\"\" \n",
    "    Calculate the impact of a non-local non-causal nudge (using a vector-nudge) \n",
    "    on the input variables on the completely causally determined output variable\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    distribution: a ProbabilityArray object\n",
    "    total_nudge_size: number\n",
    "    nudged_variables: a list of integers\n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_variables = len(distribution.probability_distribution.shape)\n",
    "    function_labels, label_nudged_variable = set([number_of_variables-1]), 0\n",
    "    input_variable_labels = set(range(number_of_variables)) - function_labels\n",
    "    input_distribution = distribution.marginalize(input_variable_labels)\n",
    "    \n",
    "    new_input_distribution = nudge.nudge_distribution_non_local_non_causal(\n",
    "        input_distribution, nudged_variables, total_nudge_size, 'random'\n",
    "    )\n",
    "    return nudge.impact_nudge_causal_output(distribution, function_labels,\n",
    "                                            new_input_distribution)\n",
    "\n",
    "number_of_variables = 5\n",
    "NUMBER_OF_STATES = 4\n",
    "TOTAL_NUDGE_SIZE = 0.01\n",
    "shape = tuple([NUMBER_OF_STATES] * (number_of_variables))\n",
    "distribution = ProbabilityArray(generate_distribution(shape, 'random_dirichlet'))\n",
    "#distribution = ProbabilityArray(generate_distribution(\n",
    "#    shape, 'fixed_entropy', \n",
    "#    {\"entropy_size\":percentage_max_entropy(shape, 0.7)}\n",
    "#))\n",
    "\n",
    "focused_nudge_impact = calculate_focused_nudge_impact(distribution, TOTAL_NUDGE_SIZE)\n",
    "print(focused_nudge_impact)\n",
    "\n",
    "nudged_variables = [0] \n",
    "vector_nudge_impact = calculate_non_local_nudge_impact(\n",
    "    distribution, TOTAL_NUDGE_SIZE, nudged_variables\n",
    ")\n",
    "print(vector_nudge_impact)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment \n",
    "\n",
    "Look at differences of impact for local vector and focused nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_variables = 4\n",
    "NUMBER_OF_STATES = 4\n",
    "TOTAL_NUDGE_SIZE = 0.01\n",
    "nudged_variables = [0]\n",
    "number_of_samples = 100\n",
    "\n",
    "focused_nudge_impacts = []\n",
    "vector_nudge_impacts = []\n",
    "RUN = False\n",
    "if RUN:\n",
    "    for i in range(number_of_samples):\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "\n",
    "        shape = tuple([NUMBER_OF_STATES] * (number_of_variables))\n",
    "        #distribution = ProbabilityArray(generate_distribution(shape, 'random_dirichlet'))\n",
    "        distribution = ProbabilityArray(generate_distribution(\n",
    "            shape, 'fixed_entropy', \n",
    "            {\"entropy_size\":percentage_max_entropy(shape, 0.7)}\n",
    "        ))        \n",
    "        focused_nudge_impact = calculate_focused_nudge_impact(distribution, TOTAL_NUDGE_SIZE)\n",
    "        focused_nudge_impacts.append(focused_nudge_impact)\n",
    "\n",
    "        vector_nudge_impact = calculate_non_local_nudge_impact(\n",
    "            distribution, TOTAL_NUDGE_SIZE, nudged_variables\n",
    "        )\n",
    "        vector_nudge_impacts.append(vector_nudge_impact)\n",
    "\n",
    "    print(\"mean & std focused nudge {} ({})\".format(\n",
    "        np.mean(focused_nudge_impacts), np.std(focused_nudge_impacts)\n",
    "    ))\n",
    "    print(\"mean & std vector nudge {} ({})\".format(\n",
    "        np.mean(vector_nudge_impacts), np.std(vector_nudge_impacts)\n",
    "    ))\n",
    "    differences = np.array(focused_nudge_impacts) - np.array(vector_nudge_impacts)\n",
    "    print(\"mean & std difference focused minus vector {} ({})\".format(\n",
    "        np.mean(differences), np.std(differences)\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment:\n",
    "The impact of a vector/focused nudge on the input variables \n",
    "(with no causal impact on the other input variables)\n",
    "on the output variable, for different number of input variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER_OF_VARIABLES, NUMBER_OF_STATES, TOTAL_NUDGE_SIZE = 5, 5, 0.01\n",
    "nudged_variables = [0]\n",
    "NUMBER_OF_SAMPLES = 10\n",
    "\n",
    "impact_focused_nudge_dict = {}\n",
    "impact_vector_nudge_dict = {}\n",
    "RUN = False\n",
    "if RUN:\n",
    "    for number_of_variables in range(1, MAX_NUMBER_OF_VARIABLES, 1):\n",
    "        print(number_of_variables)\n",
    "        focused_nudge_impacts = []\n",
    "        vector_nudge_impacts = []\n",
    "        for i in range(NUMBER_OF_SAMPLES):\n",
    "            if i%5 == 0: \n",
    "                print(\"sample number {}\".format(i))\n",
    "\n",
    "            shape = tuple([NUMBER_OF_STATES] * (number_of_variables+1))\n",
    "            #distribution = ProbabilityArray(generate_distribution(shape, 'random_dirichlet'))\n",
    "            distribution = ProbabilityArray(generate_distribution(\n",
    "                shape, 'fixed_entropy', \n",
    "                {\"entropy_size\":percentage_max_entropy(shape, 0.7)}\n",
    "            ))\n",
    "            focused_nudge_impact = calculate_focused_nudge_impact(distribution, TOTAL_NUDGE_SIZE)\n",
    "            focused_nudge_impacts.append(focused_nudge_impact)\n",
    "            vector_nudge_impact = calculate_non_local_nudge_impact(\n",
    "                distribution, TOTAL_NUDGE_SIZE, nudged_variables\n",
    "            )\n",
    "            vector_nudge_impacts.append(vector_nudge_impact)\n",
    "\n",
    "        impact_focused_nudge_dict[number_of_variables] = focused_nudge_impacts\n",
    "        impact_vector_nudge_dict[number_of_variables] = vector_nudge_impacts\n",
    "        #with open(\"back_up_number_variables_output.json\", 'w') as f:\n",
    "        #    json.dump(impact_nudge_dict, f)\n",
    "\n",
    "    #print(impact_nudge_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    plot_results(\n",
    "        (impact_focused_nudge_dict, {'mean_label': 'focused'}), \n",
    "        (impact_vector_nudge_dict, {'mean_label': 'vector'}),\n",
    "        xlabel=\"number of input variables\", ylabel = \"KL-divergence\", \n",
    "        title=\"the impact of the nudge given a number of variables\", std_of_batches=False\n",
    "    )\n",
    "    \n",
    "\n",
    "#plotting.plot_mean_and_confidence(\n",
    "#    variable_range, np.array(mean_impact_nudge)/np.array(difference_distributions), batches_std,\n",
    "#    \"std of batched means\", xlabel, \"normalised impact of the nudge\", \"normalised values\"\n",
    "#)\n",
    "\n",
    "#fit = powerlaw.Fit(mean_impact_nudge)\n",
    "#print(fit.distribution_compare(\"power_law\", \"exponential\"))\n",
    "#print(fit.distribution_compare(\"power_law\", \"lognormal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2C\n",
    "\n",
    "Compare local vector nudges to synergistic nudges (nudges on the total distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER_OF_VARIABLES, NUMBER_OF_STATES, TOTAL_NUDGE_SIZE = 5, 5, 0.01\n",
    "NUMBER_OF_SAMPLES = 50\n",
    "\n",
    "impact_local_nudge_dict = {}\n",
    "impact_synergistic_nudge_dict = {}\n",
    "RUN = False\n",
    "if RUN:\n",
    "    for number_of_input_variables in range(1, MAX_NUMBER_OF_VARIABLES, 1):\n",
    "        print(number_of_input_variables)\n",
    "        synergistic_nudge_impacts = []\n",
    "        local_nudge_impacts = []\n",
    "        for i in range(NUMBER_OF_SAMPLES):\n",
    "            if i%10 == 0: \n",
    "                print(\"sample number {}\".format(i))\n",
    "\n",
    "            nudged_variables = list(range(number_of_input_variables))\n",
    "            random.shuffle(nudged_variables)\n",
    "\n",
    "            shape = tuple([NUMBER_OF_STATES] * (number_of_input_variables+1))\n",
    "            #distribution = generate_distribution(shape, 'random_dirichlet')\n",
    "            distribution = generate_distribution(\n",
    "                shape, 'fixed_entropy', \n",
    "                {\"entropy_size\":percentage_max_entropy(shape, 0.7)}\n",
    "            )\n",
    "\n",
    "            distribution_copy = np.copy(distribution)\n",
    "\n",
    "            distribution = ProbabilityArray(distribution)\n",
    "            distribution_copy = ProbabilityArray(distribution_copy)\n",
    "\n",
    "            local_nudge_impact = calculate_non_local_nudge_impact(\n",
    "                distribution, TOTAL_NUDGE_SIZE, [0]\n",
    "            )\n",
    "            local_nudge_impacts.append(local_nudge_impact)\n",
    "\n",
    "            synergistic_nudge_impact = calculate_non_local_nudge_impact(\n",
    "                distribution_copy, TOTAL_NUDGE_SIZE, [0]\n",
    "            )\n",
    "            synergistic_nudge_impacts.append(synergistic_nudge_impact)\n",
    "            #print(\"difference local-synergistic {}\".format(local_nudge_impact-synergistic_nudge_impact))\n",
    "\n",
    "\n",
    "\n",
    "        impact_synergistic_nudge_dict[number_of_input_variables] = synergistic_nudge_impacts\n",
    "        impact_local_nudge_dict[number_of_input_variables] = local_nudge_impacts\n",
    "        #with open(\"back_up_number_variables_output.json\", 'w') as f:\n",
    "        #    json.dump(impact_nudge_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN:\n",
    "    plot_results(\n",
    "        (impact_local_nudge_dict, {'mean_label': 'local'}), \n",
    "        (impact_synergistic_nudge_dict, {'mean_label': 'synergetic'}),\n",
    "        xlabel=\"number of input variables\", ylabel = \"KL-divergence\", \n",
    "        title=\"Compare local and synergetic nudge impacts\", std_of_batches=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import probability_distributions\n",
    "import nudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_VARIABLES, NUMBER_OF_STATES = 3, 4 \n",
    "#pdf = JointProbabilityMatrix(NUMBER_OF_VARIABLES+1, NUMBER_OF_STATES, 'random')\n",
    "#initial_distribution = pdf.joint_probabilities.joint_probabilities\n",
    "shape = [NUMBER_OF_STATES]*(NUMBER_OF_VARIABLES+1)\n",
    "\n",
    "initial_distribution = generate_distribution(\n",
    "    shape, 'fixed_entropy', \n",
    "    {\"entropy_size\":percentage_max_entropy(shape, 0.75)}\n",
    ")\n",
    "\n",
    "new_distribution = nudge.mutate_distribution_with_fixed_marginals(\n",
    "    np.copy(initial_distribution), 3, 50, 0.001\n",
    ") \n",
    "\n",
    "a = probability_distributions.ProbabilityArray(initial_distribution).marginalize(set([0]))\n",
    "b = probability_distributions.ProbabilityArray(new_distribution).marginalize(set([0]))\n",
    "np.allclose(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiment 4:\n",
    "Change the output so as to minimize the nudge impact. See what happens with the mutual information between\n",
    "the nudged variable and the output distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_nudge_impact(distribution, output_label, nudge_label, number_of_nudges, local_nudge_size):\n",
    "    input_variable_labels = (set(range(len(distribution.probability_distribution.shape))) -\n",
    "                             set([output_label]))\n",
    "    input_distribution = distribution.marginalize(input_variable_labels)\n",
    "    \n",
    "    new_input_distribution = nudge.nudge_distribution_local_non_causal(\n",
    "        input_distribution, nudge_label, local_nudge_size, number_of_nudges\n",
    "    )\n",
    "    return nudge.impact_nudge_causal_output(distribution, set([output_label]),\n",
    "                                      new_input_distribution)\n",
    "\n",
    "def minimize_nudge_greedy(initial_distribution, output_label, number_of_trials, \n",
    "                          evaluations_per_trial, mutation_size, number_of_mutations,\n",
    "                          total_nudge_size, nudge_label):\n",
    "    \"\"\"\n",
    "    Mutate the distribution to minimize nudge impact and maximize entropy\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    initial_distribution: a numpy array\n",
    "        Representing a discrete probability distribution\n",
    "    function_label: an integer\n",
    "    nudge_size: a (small) number\n",
    "    number_of_nudges: an integer\n",
    "    \n",
    "    \"\"\"\n",
    "    total_number_of_states = reduce(lambda x, y: x*y, initial_distribution.shape)\n",
    "    local_nudge_size, number_of_nudges = calculate_amount_and_size_nudges(\n",
    "        total_nudge_size, total_number_of_states\n",
    "    )\n",
    "                             \n",
    "    distribution = initial_distribution\n",
    "    nudge_impacts = []\n",
    "    for i in range(evaluations_per_trial):\n",
    "        nudge_impacts.append(get_nudge_impact(\n",
    "            ProbabilityArray(initial_distribution), output_label, nudge_label, number_of_nudges, local_nudge_size\n",
    "        ))                        \n",
    "    prev_nudge_impact = np.mean(nudge_impacts)\n",
    "    initial_nudge_impact = prev_nudge_impact\n",
    "    #print(prev_nudge_impact)\n",
    "             \n",
    "    for i in range(number_of_trials):\n",
    "        #print(i)\n",
    "        #print(\"number of mutations {}\".format(number_of_mutations))\n",
    "        proposed_distribution = nudge.mutate_distribution_with_fixed_marginals(\n",
    "            distribution, output_label, int(number_of_mutations), mutation_size\n",
    "        ) \n",
    "        #print(\"found proposal distribution\")\n",
    "        nudge_impacts = []\n",
    "        for j in range(evaluations_per_trial):\n",
    "            nudge_impacts.append(get_nudge_impact(\n",
    "                ProbabilityArray(proposed_distribution), output_label, nudge_label,\n",
    "                number_of_nudges, local_nudge_size\n",
    "            ))\n",
    "        if np.mean(nudge_impacts) < prev_nudge_impact:\n",
    "            prev_nudge_impact = np.mean(nudge_impacts)\n",
    "            distribution = proposed_distribution\n",
    "            \n",
    "        #print(prev_nudge_impact)\n",
    "    \n",
    "    return distribution, prev_nudge_impact, initial_nudge_impact, prev_nudge_impact\n",
    "\n",
    "NUMBER_OF_VARIABLES, NUMBER_OF_STATES = 3, 4 \n",
    "#pdf = JointProbabilityMatrix(NUMBER_OF_VARIABLES+1, NUMBER_OF_STATES, 'random')\n",
    "#initial_distribution = pdf.joint_probabilities.joint_probabilities\n",
    "shape = [NUMBER_OF_STATES]*(NUMBER_OF_VARIABLES+1)\n",
    "initial_distribution = generate_distribution(\n",
    "    shape, 'fixed_entropy', \n",
    "    {\"entropy_size\":percentage_max_entropy(shape, 0.75)}\n",
    ")\n",
    "\n",
    "output_label = NUMBER_OF_VARIABLES\n",
    "\n",
    "number_of_trials = 50\n",
    "evaluations_per_trial = 10\n",
    "mutation_size = 0.2 / (4.0**3)\n",
    "number_of_mutations = int(0.1 * 4**3)\n",
    "a=minimize_nudge_greedy(initial_distribution, output_label, number_of_trials, \n",
    "                      evaluations_per_trial, mutation_size, number_of_mutations, 0.01, 0)\n",
    "\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_VARIABLES, NUMBER_OF_STATES = 3, 5 \n",
    "output_label = NUMBER_OF_VARIABLES\n",
    "number_of_trials = 40\n",
    "evaluations_per_trial = 500\n",
    "\n",
    "mutation_size = 0.2/(NUMBER_OF_STATES**NUMBER_OF_VARIABLES)\n",
    "number_of_mutations = int(0.1 * NUMBER_OF_STATES**NUMBER_OF_VARIABLES)\n",
    "\n",
    "NUMBER_OF_SAMPLES = 5\n",
    "PERCENTAGE_MAX_ENTROPY = 0.75\n",
    "NUDGE_SIZE = 0.01\n",
    "\n",
    "mi_before = []\n",
    "mi_after = []\n",
    "impact_before = []\n",
    "impact_after = []\n",
    "\n",
    "DATA_PATH = \"data_experiments/\"\n",
    "FILE_NAME = \"minimize_individual_focused_nudge_impact_inspect_change_MI_3var_5states.json\"\n",
    "RUN = True\n",
    "if RUN:\n",
    "    for count in range(NUMBER_OF_SAMPLES):\n",
    "        print(count)\n",
    "\n",
    "        shape = [NUMBER_OF_STATES] * (NUMBER_OF_VARIABLES+1)\n",
    "        #initial_distribution = generate_distribution(shape, 'random_dirichlet')\n",
    "        initial_distribution = generate_distribution(\n",
    "            shape, 'fixed_entropy', \n",
    "            {\"entropy_size\":percentage_max_entropy(shape, PERCENTAGE_MAX_ENTROPY)}\n",
    "        )\n",
    "\n",
    "        a=minimize_nudge_greedy(initial_distribution, output_label, number_of_trials, \n",
    "                                evaluations_per_trial, mutation_size, number_of_mutations, NUDGE_SIZE, 0)\n",
    "\n",
    "        impact_before.append(a[2])\n",
    "        impact_after.append(a[3])\n",
    "        mi_before.append(calculate_mutual_information(\n",
    "            ProbabilityArray(initial_distribution), set([0]), set([NUMBER_OF_VARIABLES])\n",
    "        ))\n",
    "        mi_after.append(calculate_mutual_information(\n",
    "            ProbabilityArray(a[0]), set([0]), set([NUMBER_OF_VARIABLES])\n",
    "        ))\n",
    "        with open(DATA_PATH + FILE_NAME, 'w') as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"impact_before\": impact_before,\n",
    "                    \"impact_after\": impact_after,\n",
    "                    \"mi_before\": mi_before,\n",
    "                    \"mi_after\": mi_after\n",
    "                },\n",
    "                f,\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "    print(\"mean impact before {}\".format(np.mean(impact_before)))\n",
    "    print(\"mean impact before {}\".format(np.mean(impact_after)))\n",
    "\n",
    "    print(\"mean mi before {}\".format(np.mean(mi_before)))\n",
    "    print(\"mean mi after {}\".format(np.mean(mi_after)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_before = 4.26328397408e-06\n",
    "impact_after = 2.55118132475e-06\n",
    "\n",
    "mi_before = 0.0169666085862\n",
    "mi_after = 0.0158801512352\n",
    "\n",
    "expected_change = (impact_before-impact_after)*4261\n",
    "print(\"expected change {}\".format(expected_change))\n",
    "expected_mi_after = mi_before - expected_change \n",
    "print(expected_mi_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tryout cell to generate a joint with a certain entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = tuple([5, 5, 5, 5, 5])\n",
    "entropy_size = percentage_states_max_entropy(shape, 0.1)\n",
    "print(entropy_size)\n",
    "dist = generate_distribution(shape, method='fixed_entropy', arguments={\"entropy_size\": entropy_size})\n",
    "print(entropy(dist.flatten(), base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiment 5 \n",
    "\n",
    "Now assume that the nudge on the local variable caually impacts the other input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_input_variables = 3\n",
    "number_of_states = 5\n",
    "nudge_size = 0.01\n",
    "\n",
    "input_distribution = generate_distribution()\n",
    "nudge_labels = [0]\n",
    "other_input_labels = list(range(1, number_of_input_variables))\n",
    "\n",
    "def calculate_new_distribution_after_nudge(distribution, nudge_labels, other_variable_labels):\n",
    "    \"\"\"\n",
    "    Perform a nudge on a subset of the variables and assume\n",
    "    they causally impact the other variables, and return the new\n",
    "    distribution\n",
    "    \n",
    "    Note:\n",
    "    ----\n",
    "    If the nudged variables contain 0 states than those states are removed\n",
    "    from the entire distribution\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    distribution: a numpy array\n",
    "    nudge_labels: A list of integers\n",
    "        The variables that will be nudged\n",
    "    other_variable_labels: A list of integers\n",
    "    \n",
    "    Returns: a numpy array\n",
    "    -------\n",
    "    The new distribution\n",
    "    \n",
    "    \"\"\"\n",
    "    marginal_nudge = ProbabilityArray(distribution).marginalize(set(nudge_labels))\n",
    "    if np.all(marginal_nudge!=0):\n",
    "        conditional_on_nudge, other_labels, nudge_labels_old = (\n",
    "            ProbabilityArray(input_distribution).find_conditional(set(other_labels), set(nudge_labels))\n",
    "        )\n",
    "    else:\n",
    "        #adjust this to multiple nudge on input states\n",
    "        \n",
    "        for label in nudge_labels:\n",
    "            zero_states = [count for count, state in enumerate(marginal_nudge) if state == 0]\n",
    "            for zero_state in zero_states:\n",
    "                input_distribution = np.delete(input_distribution, zero_state, nudge_label)\n",
    "\n",
    "        marginal_nudge = ProbabilityArray(input_distribution).marginalize(set(nudge_labels))\n",
    "        conditional_on_nudge, other_labels, nudge_labels_old = (\n",
    "            ProbabilityArray(input_distribution).find_conditional(set(other_input_labels), set(nudge_labels))\n",
    "        )\n",
    "\n",
    "    marginal_nudged, nudges_states = nudge.nudge(\n",
    "        marginal_variable_old, nudge_size\n",
    "    )\n",
    "    input_distribution_new = probability_distributions.compute_joint(\n",
    "        marginal_nudged, conditional_on_nudge, nudge_labels_old\n",
    "    ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiment 6\n",
    "#### find the maximal local non-causal additive nudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nudge_impact(old_input, new_input, conditional_output, measure=\"absolute\"):\n",
    "    \"\"\"\n",
    "    Find the impact of a nudge transforming the old input into\n",
    "    the new input.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    old_input: nd-array\n",
    "        representing a probability distribution\n",
    "    new_input: nd-array\n",
    "        representing a probability distribution\n",
    "    conditional_output: nd-array\n",
    "        represening the probability distribution, the last variable\n",
    "        should be the conditional output (in tree form the leaves should be\n",
    "        the conditional output)\n",
    "    measure: string\n",
    "        Should be in: {\"absolute\", \"kl-divergence\"}\n",
    "        \n",
    "    Returns: a number\n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_input_vars = len(old_input.shape)\n",
    "    old_joint = probability_distributions.compute_joint(\n",
    "        old_input, conditional_output, set(range(0, number_of_input_vars, 1))\n",
    "    )\n",
    "    print(\"old joint {}\".format(old_joint))\n",
    "    old_output = ProbabilityArray(old_joint).marginalize(set([number_of_input_vars]))\n",
    "    new_joint = probability_distributions.compute_joint(\n",
    "        new_input, conditional_output, set(range(0, number_of_input_vars, 1))\n",
    "    )\n",
    "    new_output = ProbabilityArray(new_joint).marginalize(set([number_of_input_vars]))\n",
    "    if measure==\"absolute\":\n",
    "        return np.sum(np.absolute(old_output.flatten()-new_output.flatten()))\n",
    "    elif measure==\"kl-divergence\":\n",
    "        return entropy(old_output.flatten(), new_output.flatten(), base=2)\n",
    "    else:\n",
    "        raise ValueError(\"provide a valid measure\")\n",
    "        \n",
    "old_input = np.array([0.1, 0.2, 0.05, 0.3, 0.35])\n",
    "new_input = np.array([0.12, 0.18, 0.06, 0.31, 0.33])\n",
    "cond_output = np.array([\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.2, 0.5, 0.3],\n",
    "    [0.15, 0.45, 0.4],\n",
    "    [0.46, 0.18, 0.36],\n",
    "    [0.32, 0.43, 0.25]\n",
    "])\n",
    "find_nudge_impact(old_input, new_input, cond_output)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First do it for distributions randomly drawn from the entire space of distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maximum_nudges\n",
    "from maximum_nudges import find_maximum_local_nudge\n",
    "from maximum_nudges import find_max_control_impact\n",
    "import nudge_new\n",
    "\n",
    "amount_of_states = 5\n",
    "amount_of_input_vars = 5\n",
    "number_of_samples = 50\n",
    "nudge_size = 0.01\n",
    "\n",
    "input_vars_to_random_local_impacts = {}\n",
    "input_vars_to_max_local_impacts = {}\n",
    "input_vars_to_random_control_impacts = {}\n",
    "input_vars_to_max_control_impacts = {}\n",
    "for input_vars in range(1, amount_of_input_vars+1, 1):\n",
    "    print(input_vars)\n",
    "    random_local_nudge_impacts = []\n",
    "    max_local_nudge_impacts = []\n",
    "    random_control_nudge_impacts = []\n",
    "    max_control_nudge_impacts = []\n",
    "    for i in range(number_of_samples):\n",
    "        shape = [amount_of_states]*(input_vars+1)\n",
    "        joint_distribution = generate_distribution(shape, \"random_dirichlet\")\n",
    "        input_dist = ProbabilityArray(joint_distribution).marginalize(set(list(range(input_vars))))\n",
    "        cond_output, output_label, input_labels = ProbabilityArray(joint_distribution).find_conditional(\n",
    "            set([input_vars]), set(list(range(input_vars)))\n",
    "        )\n",
    "\n",
    "        #find the nudge impacts\n",
    "        input_locally_nudged = nudge_new.local_non_causal(input_dist, nudge_size)\n",
    "        random_local_impact = nudge_new.find_nudge_impact(\n",
    "            input_dist, input_locally_nudged, cond_output\n",
    "        )\n",
    "        random_local_nudge_impacts.append(random_local_impact)\n",
    "        \n",
    "        temp_cond_output = np.reshape(cond_output, (amount_of_states**input_vars, amount_of_states))\n",
    "        #tryout\n",
    "        max_local_impact1 = maximum_nudges.find_maximum_local_nudge_without_conditional(\n",
    "            input_dist, temp_cond_output, nudge_size\n",
    "        )\n",
    "        max_local_impact = find_maximum_local_nudge(\n",
    "            input_dist, temp_cond_output, nudge_size\n",
    "        )\n",
    "        if abs(max_local_impact-max_local_impact1) > 10**(-7):\n",
    "            raise ValueError()\n",
    "\n",
    "        max_local_nudge_impacts.append(max_local_impact)\n",
    "        \n",
    "        input_control_nudged = nudge_new.control_non_causal(input_dist, nudge_size)\n",
    "        random_control_impact = nudge_new.find_nudge_impact(\n",
    "            input_dist, input_control_nudged, cond_output\n",
    "        )\n",
    "        random_control_nudge_impacts.append(random_control_impact)\n",
    "        \n",
    "        _, _, max_control_impact = find_max_control_impact(\n",
    "            input_dist, cond_output, nudge_size\n",
    "        )\n",
    "        max_control_nudge_impacts.append(max_control_impact)\n",
    "        \n",
    "    input_vars_to_random_local_impacts[input_vars] = random_local_nudge_impacts\n",
    "    input_vars_to_max_local_impacts[input_vars] = max_local_nudge_impacts\n",
    "    input_vars_to_random_control_impacts[input_vars] = random_control_nudge_impacts\n",
    "    input_vars_to_max_control_impacts[input_vars] = max_control_nudge_impacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_range, mean_random_local, std_random_local, batch_std_random_local = find_mean_std_mse(\n",
    "    input_vars_to_random_local_impacts, batch_size=5\n",
    ")\n",
    "plot_range, mean_max_local, std_max_local, batch_std_max_local = find_mean_std_mse(\n",
    "    input_vars_to_max_local_impacts, batch_size=5\n",
    ")\n",
    "plot_range, mean_random_control, std_random_control, batch_std_random_control = find_mean_std_mse(\n",
    "    input_vars_to_random_control_impacts, batch_size=5\n",
    ")\n",
    "plot_range, mean_max_control, std_max_control, batch_std_max_control = find_mean_std_mse(\n",
    "    input_vars_to_max_control_impacts, batch_size=5\n",
    ")\n",
    "\n",
    "plt.plot(plot_range, mean_random_local, label=\"random local\")\n",
    "plt.plot(plot_range, mean_max_local, label=\"max local\")\n",
    "plt.plot(plot_range, mean_random_control, label=\"random control\")\n",
    "plt.plot(plot_range, mean_max_control, label=\"max control\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plot_range, mean_random_local, label=\"random local\")\n",
    "plt.plot(plot_range, mean_random_control, label=\"rendom control\")\n",
    "\n",
    "lower_bound_random_local = np.array(mean_random_local)-np.array(std_random_local)\n",
    "upper_bound_random_local = np.array(mean_random_local)+np.array(std_random_local)\n",
    "plt.fill_between(plot_range, lower_bound_random_local, upper_bound_random_local, \n",
    "                 label='{}'.format(\"random local std\"), alpha=0.2)\n",
    "\n",
    "lower_bound_random_control = np.array(mean_random_control)-np.array(std_random_control)\n",
    "upper_bound_random_control = np.array(mean_random_control)+np.array(std_random_control)\n",
    "plt.fill_between(plot_range, lower_bound_random_control, upper_bound_random_control, \n",
    "                 label='{}'.format(\"random control std\"), alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot_results(\n",
    "#    (input_vars_to_max_impacts, {'mean_label': 'max_local_nudges'}), \n",
    "#    xlabel=\"number of input variables\", ylabel = \"absolute impact\", \n",
    "#    title=\"maximum local non-causal additive nudges\", std_of_batches=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now do it for distributions with a limited entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maximum_nudges\n",
    "from maximum_nudges import find_maximum_local_nudge\n",
    "from maximum_nudges import find_max_control_impact\n",
    "import nudge_new\n",
    "import evolutionary_algorithms as ea\n",
    "from probability_distributions import produce_distribution_with_entropy_evolutionary_old as get_entropy_dist\n",
    "\n",
    "amount_of_states = 5\n",
    "amount_of_input_vars = 4\n",
    "number_of_samples = 5\n",
    "nudge_size = 0.01\n",
    "entropy_amount = 0.9\n",
    "\n",
    "input_vars_to_random_local_impacts = {}\n",
    "input_vars_to_max_local_impacts = {}\n",
    "input_vars_to_random_control_impacts = {}\n",
    "input_vars_to_max_control_impacts = {}\n",
    "for input_vars in range(1, amount_of_input_vars+1, 1):\n",
    "    print(input_vars)\n",
    "    random_local_nudge_impacts = []\n",
    "    max_local_nudge_impacts = []\n",
    "    random_control_nudge_impacts = []\n",
    "    max_control_nudge_impacts = []\n",
    "    for i in range(number_of_samples):\n",
    "        #if i%2==0 and input_vars>2:\n",
    "        #    print(i)\n",
    "        #shape = [amount_of_states]*(input_vars+1)\n",
    "        #joint_distribution = generate_distribution(shape, \"fixed_entropy\", {\"entropy_size\": 0.9})\n",
    "        #input_dist = ProbabilityArray(joint_distribution).marginalize(set(list(range(input_vars))))\n",
    "        #cond_output, output_label, input_labels = ProbabilityArray(joint_distribution).find_conditional(\n",
    "        #    set([input_vars]), set(list(range(input_vars)))\n",
    "        #)\n",
    "\n",
    "        input_shape = [amount_of_states]*(input_vars)\n",
    "        max_entropy = np.log2(amount_of_states**input_vars)\n",
    "        entropy_size = max_entropy * entropy_amount\n",
    "        input_dist = get_entropy_dist(tuple(input_shape), entropy_size, 1200, initial_dist=\"random\")\n",
    "        print(\"percentage max entropy dist {}\".format(entropy(input_dist, base=2)/max_entropy))\n",
    "        input_dist = np.reshape(input_dist, tuple(input_shape))\n",
    "        #input_dist = generate_distribution(input_shape, \"fixed_entropy\", {\"entropy_size\": entropy_amount})\n",
    "        #input_dist = generate_distribution(input_shape, 'random_dirichlet')\n",
    "        cond_shape = [amount_of_states]*(input_vars+1)\n",
    "        cond_output = [\n",
    "            probability_distributions.compute_joint_uniform_random((amount_of_states,))\n",
    "            for i in range(amount_of_states**(input_vars))\n",
    "        ]\n",
    "        cond_output = np.array(cond_output)\n",
    "        cond_output = np.reshape(cond_output, cond_shape)\n",
    "\n",
    "        #find the nudge impacts\n",
    "        input_locally_nudged = nudge_new.local_non_causal_without_conditional(input_dist, nudge_size)\n",
    "        random_local_impact = nudge_new.find_nudge_impact(\n",
    "            input_dist, input_locally_nudged, cond_output\n",
    "        )\n",
    "        random_local_nudge_impacts.append(random_local_impact)\n",
    "        \n",
    "        temp_cond_output = np.reshape(cond_output, (amount_of_states**input_vars, amount_of_states))\n",
    "        max_local_impact = maximum_nudges.find_maximum_local_nudge_without_conditional(\n",
    "            input_dist, temp_cond_output, nudge_size\n",
    "        )\n",
    "        max_local_nudge_impacts.append(max_local_impact)\n",
    "        \n",
    "        input_control_nudged = nudge_new.control_non_causal(input_dist, nudge_size, True)\n",
    "        random_control_impact = nudge_new.find_nudge_impact(\n",
    "            input_dist, input_control_nudged, cond_output\n",
    "        )\n",
    "        random_control_nudge_impacts.append(random_control_impact)\n",
    "        \n",
    "        _, _, max_control_impact = find_max_control_impact(\n",
    "            input_dist, cond_output, nudge_size\n",
    "        )\n",
    "        max_control_nudge_impacts.append(max_control_impact)\n",
    "        \n",
    "    input_vars_to_random_local_impacts[input_vars] = random_local_nudge_impacts\n",
    "    input_vars_to_max_local_impacts[input_vars] = max_local_nudge_impacts\n",
    "    input_vars_to_random_control_impacts[input_vars] = random_control_nudge_impacts\n",
    "    input_vars_to_max_control_impacts[input_vars] = max_control_nudge_impacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_range, mean_random_local, std_random_local, batch_std_random_local = find_mean_std_mse(\n",
    "    input_vars_to_random_local_impacts, batch_size=5\n",
    ")\n",
    "plot_range, mean_max_local, std_max_local, batch_std_max_local = find_mean_std_mse(\n",
    "    input_vars_to_max_local_impacts, batch_size=5\n",
    ")\n",
    "plot_range, mean_random_control, std_random_control, batch_std_random_control = find_mean_std_mse(\n",
    "    input_vars_to_random_control_impacts, batch_size=5\n",
    ")\n",
    "plot_range, mean_max_control, std_max_control, batch_std_max_control = find_mean_std_mse(\n",
    "    input_vars_to_max_control_impacts, batch_size=5\n",
    ")\n",
    "\n",
    "lower_bound_max_local = np.array(mean_max_local)-np.array(std_random_local)\n",
    "upper_bound_max_local = np.array(mean_max_local)+np.array(std_random_local)\n",
    "lower_bound_max_control = np.array(mean_max_control)-np.array(std_random_control)\n",
    "upper_bound_max_control = np.array(mean_max_control)+np.array(std_random_control)\n",
    "\n",
    "lower_bound_random_local = np.array(mean_random_local)-np.array(std_random_local)\n",
    "upper_bound_random_local = np.array(mean_random_local)+np.array(std_random_local)\n",
    "lower_bound_random_control = np.array(mean_random_control)-np.array(std_random_control)\n",
    "upper_bound_random_control = np.array(mean_random_control)+np.array(std_random_control)\n",
    "\n",
    "plt.plot(plot_range, mean_random_local, label=\"random local\")\n",
    "plt.plot(plot_range, mean_max_local, label=\"max local\")\n",
    "plt.plot(plot_range, mean_random_control, label=\"random control\")\n",
    "plt.plot(plot_range, mean_max_control, label=\"max control\")\n",
    "\n",
    "plt.fill_between(plot_range, lower_bound_random_local, upper_bound_random_local, \n",
    "                 label='{}'.format(\"random local std\"), alpha=0.2)\n",
    "plt.fill_between(plot_range, lower_bound_random_control, upper_bound_random_control, \n",
    "                 label='{}'.format(\"random control std\"), alpha=0.2)\n",
    "plt.fill_between(plot_range, lower_bound_max_local, upper_bound_max_local, \n",
    "                 label='{}'.format(\"random local std\"), alpha=0.2)\n",
    "plt.fill_between(plot_range, lower_bound_max_control, upper_bound_max_control, \n",
    "                 label='{}'.format(\"random control std\"), alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(plot_range, mean_random_local, label=\"random local\")\n",
    "plt.plot(plot_range, mean_random_control, label=\"rendom control\")\n",
    "\n",
    "plt.fill_between(plot_range, lower_bound_random_local, upper_bound_random_local, \n",
    "                 label='{}'.format(\"random local std\"), alpha=0.2)\n",
    "plt.fill_between(plot_range, lower_bound_random_control, upper_bound_random_control, \n",
    "                 label='{}'.format(\"random control std\"), alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot_results(\n",
    "#    (input_vars_to_max_impacts, {'mean_label': 'max_local_nudges'}), \n",
    "#    xlabel=\"number of input variables\", ylabel = \"absolute impact\", \n",
    "#    title=\"maximum local non-causal additive nudges\", std_of_batches=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probability_distributions import produce_distribution_with_entropy_evolutionary as get_entropy_dist\n",
    "\n",
    "shape = tuple([5,5,5,5,5])\n",
    "entropy_size = np.log2(5**5) * 0.95\n",
    "print(\"the wanted entropy {}\".format(entropy_size))\n",
    "dist = get_entropy_dist(shape, entropy_size, 500)\n",
    "print(entropy(dist, base=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cond_nudge = np.array([\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6]\n",
    "    ],\n",
    "    [\n",
    "        [7,8,9],\n",
    "        [10,11,12]\n",
    "    ]\n",
    "])\n",
    "a = np.take(cond_nudge, 1, axis=2).flatten()\n",
    "print(range(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
