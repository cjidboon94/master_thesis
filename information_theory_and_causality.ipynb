{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information theory and nudge impact\n",
    "\n",
    "This notebook has as purpose to research the relation between information theoretical measures \n",
    "(mostly mutual information) and nudge impact of different nudges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from jointpdf.jointpdf import JointProbabilityMatrix\n",
    "from jointpdf.jointpdf import NestedArrayOfProbabilities\n",
    "from jointpdf.jointpdf import FullNestedArrayOfProbabilities\n",
    "from extension_probability_matrix import JointProbabilityMatrixExtended\n",
    "\n",
    "import probability_distributions\n",
    "from probability_distributions import ProbabilityArray\n",
    "import nudge\n",
    "import plotting\n",
    "import simulate\n",
    "from information_theory import calculate_mutual_information\n",
    "import information_theory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_amount_and_size_nudges(total_nudge_size, number_of_states, threshold=10):\n",
    "    \"\"\"\n",
    "    Calculate the nudge size and the number of nudges that need to be performed \n",
    "    to nudge a variable with the total nudge size. Assuming the distribution is\n",
    "    not too peaked, in other words, not too many states should have a probability\n",
    "    that is 10 times smaller than normal.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    total_nudge_size: a number\n",
    "        How much the variable need to be nudged\n",
    "    number_of_states: a number\n",
    "        The total number of states of the joint distribution\n",
    "    threshold: a float \n",
    "        Indicating how much smaller than uniform the value of the number\n",
    "        at the 95-99 percentile of points is. Defaults to 10 \n",
    "        \n",
    "    Returns: local_nudge, number_of_nudges\n",
    "    -------\n",
    "    local_nudge: a number \n",
    "        The size of the local nudge to be performed on the joint distribution\n",
    "    number_of_nudges: integer\n",
    "        How often the nudge need to be performed\n",
    "    \n",
    "    \"\"\"\n",
    "    assumed_min_size = 1.0/threshold\n",
    "    max_local_nudge = min(total_nudge_size, 0.1/number_of_states)\n",
    "    number_of_nudges = int(np.ceil(total_nudge_size/max_local_nudge))\n",
    "    local_nudge = total_nudge_size/float(number_of_nudges) \n",
    "    return local_nudge, number_of_nudges\n",
    "\n",
    "def generate_distribution(shape, method, arguments=None):\n",
    "    if method=='random_biased':\n",
    "        distribution = np.random.random(shape)\n",
    "        distribution = distribution/np.sum(distribution)\n",
    "        return distribution\n",
    "    elif method=='random_dirichlet':\n",
    "        return probability_distributions.compute_joint_uniform_random(shape)\n",
    "    elif method=='fixed_entropy':\n",
    "        return probability_distributions.generate_probability_distribution_with_certain_entropy(\n",
    "            shape, arguments['entropy_size']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('provide a valid method')\n",
    "        \n",
    "def percentage_max_entropy(shape, percentage):\n",
    "    \"\"\" \n",
    "    Return the percentage of the max-entropy given the shape\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    shape: iterable\n",
    "    percentage: float\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.log2(reduce(lambda x,y: x*y, shape)) * percentage\n",
    "\n",
    "def percentage_states_max_entropy(shape, percentage):\n",
    "    \"\"\" \n",
    "    Return the percentage of the max-entropy given the shape\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    shape: iterable\n",
    "    percentage: float\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.log2(reduce(lambda x,y: x*y, shape) * percentage)\n",
    "\n",
    "def transfer_ndarray_to_joint_probability_matrix(array):\n",
    "    \"\"\"\n",
    "    array: nd-array\n",
    "        All axis should have the same size\n",
    "        \n",
    "    \"\"\"\n",
    "    shape = array.shape\n",
    "    if not all([item==shape[0] for item in shape]):\n",
    "        raise ValueError(\"all variables need to have same number of states\")\n",
    "    \n",
    "    number_of_variables = len(shape) \n",
    "    number_of_states = shape[0]\n",
    "    return JointProbabilityMatrix(number_of_variables, number_of_states, array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 1:\n",
    "\n",
    "How do mutual information and nudge impact relate for one input variable and one output variable.\n",
    "The joint of the input and output variables is generated randomly biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_of_nudge_1d(distribution, nudge_size):\n",
    "    \"\"\"\n",
    "    Nudge the input variable and calculate the effect on the output variable\n",
    "    (the KL-devergence of the output variable)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    distribution: a numpy array\n",
    "        It should represent the joint probability distribution of 1 input\n",
    "        (the first axis) and 1 output variable (the second axis).\n",
    "    nudge_size: a number\n",
    "    \n",
    "    Returns: a number\n",
    "    \"\"\"\n",
    "    probability_array_old = ProbabilityArray(distribution)\n",
    "    marginal_variable_old = probability_array_old.marginalize(set([0]))\n",
    "    marginal_function_old = probability_array_old.marginalize(set([1]))\n",
    "    conditional_joint_old, marginal_labels_old, conditional_labels_old = (\n",
    "        probability_array_old.find_conditional(set([1]), set([0]))\n",
    "    )\n",
    "    marginal_variable_nudged, nudges_states = nudge.nudge(\n",
    "        marginal_variable_old, nudge_size\n",
    "    )\n",
    "    joint_new = ProbabilityArray(probability_distributions.compute_joint(\n",
    "        marginal_variable_nudged, conditional_joint_old, conditional_labels_old\n",
    "    ))\n",
    "    marginal_function_new = joint_new.marginalize(set([1]))  \n",
    "    kl_variable = entropy(marginal_variable_old, marginal_variable_nudged)\n",
    "    kl_function = entropy(marginal_function_old, marginal_function_new) \n",
    "    return kl_variable, kl_function\n",
    "\n",
    "#this is a biased result replace by dirichlet distribution\n",
    "pdf = JointProbabilityMatrix(1, 10, 'random')\n",
    "\n",
    "#non-biased result\n",
    "number_of_input_vars = 1\n",
    "number_of_states = 5\n",
    "dist = np.random.dirichlet([1] * number_of_states**number_of_input_vars)\n",
    "pdf = transfer_ndarray_to_joint_probability_matrix(dist)\n",
    "pdf.append_variables_with_target_mi(1, 0.5)\n",
    "distribution = pdf.joint_probabilities.joint_probabilities\n",
    "effect_of_nudge_1d(distribution, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform actual experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see whether and how mutual information and response to the nudge co-depend\n",
    "NUMBER_OF_STATES, NUDGE_SIZE = 5, 0.01\n",
    "mutual_information_sizes = np.arange(0.05, 1, 0.05)\n",
    "sample_size = 100\n",
    "effect_nudge_given_mi = {}\n",
    "for mutual_information_size in mutual_information_sizes:\n",
    "    print(\"the mutual information size is {}\".format(mutual_information_size))\n",
    "    nudge_effects = []\n",
    "    for sample in range(sample_size):\n",
    "        pdf = JointProbabilityMatrix(1, NUMBER_OF_STATES, 'random')\n",
    "        pdf.append_variables_with_target_mi(1, mutual_information_size)\n",
    "        distribution = pdf.joint_probabilities.joint_probabilities\n",
    "        nudge_effects.append(effect_of_nudge_1d(distribution, 0.01)[1])\n",
    "        \n",
    "    effect_nudge_given_mi[mutual_information_size] = nudge_effects\n",
    "    with open(\"relation_MI_nudge_impact_1input.json\", 'w') as f:\n",
    "        json.dump(effect_nudge_given_mi, f)\n",
    "    \n",
    "#print(effect_nudge_given_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range, mean, std, batches_stc = simulate.find_mean_std_mse(\n",
    "    effect_nudge_given_mi, \n",
    ")\n",
    "print(mean)\n",
    "import scipy\n",
    "print()\n",
    "scipy.stats.linregress(plot_range, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_1_random_input_1_output_diff_MI.json\", 'r') as f:\n",
    "    first = json.load(f)\n",
    "\n",
    "effect_nudge_given_mi = {}\n",
    "    \n",
    "for k, v in first.items():\n",
    "    effect_nudge_given_mi[float(k)] = v\n",
    "    \n",
    "average_effect_nudge_dict = {k:np.mean(v) for k,v in effect_nudge_given_mi.items()}\n",
    "standard_deviation_effect_nudge_dict = {k:np.std(v) for k,v in effect_nudge_given_mi.items()}\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "batches_mean_squared_error = {}\n",
    "for mi, effect_nudge_list in effect_nudge_given_mi.items():\n",
    "    batched_estimates = []\n",
    "    for i in range(len(effect_nudge_list)/BATCH_SIZE):\n",
    "        batched_estimates.append(\n",
    "            np.mean(effect_nudge_list[i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "        )\n",
    "    batches_mean_squared_error[mi] = np.std(batched_estimates)\n",
    "\n",
    "batch_std_effect_nudge_ord_dict = collections.OrderedDict(\n",
    "    sorted(batches_mean_squared_error.items(), key= lambda x: x[0])\n",
    ")    \n",
    "average_effect_nudge_ord_dict = collections.OrderedDict(\n",
    "    sorted(average_effect_nudge_dict.items(), key= lambda x: x[0])\n",
    ")\n",
    "std_effect_nudge_ord_dict = collections.OrderedDict(\n",
    "    sorted(standard_deviation_effect_nudge_dict.items(), key= lambda x: x[0])\n",
    ")\n",
    "\n",
    "mi_values = average_effect_nudge_ord_dict.keys()\n",
    "mean_effect_nudge = average_effect_nudge_ord_dict.values()\n",
    "std_effect_nudge = std_effect_nudge_ord_dict.values()\n",
    "batch_std_effect_nudge = batch_std_effect_nudge_ord_dict.values()\n",
    "\n",
    "xlabel = \"mutual information\"\n",
    "ylabel = \"effect of the nudge\"\n",
    "title = \"Effect of a nudge on input variable on output variable for certain MI\"\n",
    "plotting.plot_mean_and_confidence(\n",
    "    mi_values, mean_effect_nudge, std_effect_nudge,\n",
    "    \"std\", xlabel, ylabel, title\n",
    ")\n",
    "plotting.plot_mean_and_confidence(\n",
    "    mi_values, mean_effect_nudge, batch_std_effect_nudge,\n",
    "    \"MSE\", xlabel, ylabel, title\n",
    ")\n",
    "\n",
    "mi_values1, mean_effect_nudge1, std_effect_nudge1, batch_std_effect_nudge1 = (\n",
    "    simulate.find_mean_std_mse(effect_nudge_given_mi, batch_size=30)\n",
    ")\n",
    "\n",
    "#print(np.allclose(mi_values1, mi_values))\n",
    "#print(np.allclose(mean_effect_nudge1, mean_effect_nudge))\n",
    "#print(np.allclose(std_effect_nudge1, std_effect_nudge))\n",
    "#print(np.allclose(batch_std_effect_nudge1, batch_std_effect_nudge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "\n",
    "#### mutual information between output and individual input for fixed total entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment:\n",
    "The relation between nudge impact and the mutual information between the output variable and\n",
    "the nudged input variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_VARIABLES, NUMBER_OF_STATES, TOTAL_NUDGE_SIZE = 4, 5, 0.01\n",
    "shape = tuple([NUMBER_OF_STATES]*(NUMBER_OF_VARIABLES+1))\n",
    "NUMBER_OF_SAMPLES = 100\n",
    "\n",
    "local_nudge, number_of_nudges = calculate_amount_and_size_nudges(\n",
    "    TOTAL_NUDGE_SIZE, NUMBER_OF_STATES**NUMBER_OF_VARIABLES\n",
    ")\n",
    "impact_nudges_and_mi = []\n",
    "for i in range(NUMBER_OF_SAMPLES):\n",
    "    if i%2==0 and i != 0:\n",
    "        print(\"sample number {}\".format(i))\n",
    "    \n",
    "    #calculate the distribution\n",
    "    #distribution = ProbabilityArray(generate_distribution(shape, 'random_dirichlet'))\n",
    "    distribution = ProbabilityArray(generate_distribution(\n",
    "            shape, 'fixed_entropy', \n",
    "            {\"entropy_size\":percentage_max_entropy(shape, 0.75)}\n",
    "    ))\n",
    "    function_label, label_nudged_variable = NUMBER_OF_VARIABLES, 0\n",
    "    function_labels = set([function_label])\n",
    "    input_variable_labels = set(range(len(distribution.probability_distribution.shape))) - function_labels\n",
    "    \n",
    "    #calculate mutual information\n",
    "    mutual_information = calculate_mutual_information(distribution, \n",
    "                                                      set([function_label]),\n",
    "                                                      set([label_nudged_variable]))\n",
    "    \n",
    "    #calculate_nudge_impact\n",
    "    input_distribution = distribution.marginalize(input_variable_labels)\n",
    "    nudge_impacts = []\n",
    "    for _ in range(1000):\n",
    "        new_input_distribution = nudge.nudge_distribution_local_non_causal(\n",
    "            input_distribution, 0, local_nudge, number_of_nudges\n",
    "        )\n",
    "        nudge_impacts.append(nudge.impact_nudge_causal_output(\n",
    "            distribution, function_labels, new_input_distribution\n",
    "        ))\n",
    "    nudge_impact = np.mean(nudge_impacts)\n",
    "    impact_nudges_and_mi.append((nudge_impact, mutual_information))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "impact_nudges = [item[0] for item in impact_nudges_and_mi] \n",
    "mutual_information_sizes = [item[1] for item in impact_nudges_and_mi]\n",
    "plt.plot(mutual_information_sizes, impact_nudges, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.linregress(mutual_information_sizes, impact_nudges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
