{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all the relevant scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#general\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for the data\n",
    "import generate_data as generator\n",
    "from data.cifar10_utils import transform_label_encoding_to_one_hot\n",
    "\n",
    "#for the network\n",
    "from feedforward_ANN.cifar10_utils import get_cifar10_raw_data, preprocess_cifar10_data\n",
    "from feedforward_ANN.cifar10_utils import transform_label_encoding_to_one_hot\n",
    "from feedforward_ANN.layer import LinearLayer, ReLuLayer, TanHLayer\n",
    "from feedforward_ANN.network import Network\n",
    "from feedforward_ANN.train import SGD\n",
    "from feedforward_ANN.loss import SoftmaxCrossEntropyLoss\n",
    "from feedforward_ANN.score import get_accuracy as get_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data parameters\n",
    "grid_len, grid_width = 16, 16\n",
    "train_size = 10000\n",
    "validation_size = 5000\n",
    "test_size = 1000\n",
    "\n",
    "params = {\n",
    "    \"size_square_li\" : [4, 5, 6],\n",
    "    \"radius_circle_li\" : [2.5, 3, 3.5, 4],\n",
    "    \"length_rectangle_li\" : [6, 7, 8],\n",
    "    \"width_rectangle_li\" : [2.5, 3.5],\n",
    "    \"rotation_li_rectangle\" : [0, 45, 90, 135],\n",
    "    \"rotation_li_square\" : [0, 45],\n",
    "    \"distance_border\" : int(2)\n",
    "}\n",
    "    \n",
    "gen = generator.GenerateData(grid_len, grid_width, params=params)\n",
    "\n",
    "all_pictures = gen.generate_all_data(params[\"distance_border\"])\n",
    "pictures = {\n",
    "    \"circle\": gen.generate_all_data(params[\"distance_border\"], shape=\"circle\"),\n",
    "    \"square\": gen.generate_all_data(params[\"distance_border\"], shape=\"square\"),\n",
    "    \"rectangle\": gen.generate_all_data(params[\"distance_border\"], shape=\"rectangle\")\n",
    "}\n",
    "\n",
    "if sum([pictures[cat].shape[0] for cat in gen.category_li]) != all_pictures.shape[0]:\n",
    "    print(\"There is overlap between the classes look at your parameters!!!\")\n",
    "\n",
    "print(all_pictures.shape)\n",
    "for category in gen.category_li:\n",
    "    print(\"the {} has shape {}\".format(category, pictures[category].shape[0]))\n",
    "\n",
    "min_length_category = min(\n",
    "    [pictures[cat].shape[0] for cat in gen.category_li]\n",
    ")\n",
    "\n",
    "for category in gen.category_li:\n",
    "    np.random.shuffle(pictures[category])\n",
    "\n",
    "all_pictures_plus_labels = np.zeros((min_length_category*3, pictures[\"circle\"].shape[1]+1))\n",
    "all_pictures_plus_labels[:min_length_category, -1:] = 0\n",
    "all_pictures_plus_labels[min_length_category:2*min_length_category, -1:] = 1\n",
    "all_pictures_plus_labels[2*min_length_category:, -1:] = 2\n",
    "\n",
    "all_pictures_plus_labels[:min_length_category, :-1] = (\n",
    "    pictures[\"circle\"][:min_length_category, :]\n",
    ")\n",
    "all_pictures_plus_labels[min_length_category:2*min_length_category, :-1] = (\n",
    "    pictures[\"square\"][:min_length_category, :]\n",
    ")\n",
    "all_pictures_plus_labels[2*min_length_category:, :-1] = (\n",
    "    pictures[\"rectangle\"][:min_length_category, :]\n",
    ")\n",
    "\n",
    "np.random.shuffle(all_pictures_plus_labels)\n",
    "\n",
    "num_classes = 3\n",
    "size_training_set = int(0.8 * all_pictures_plus_labels.shape[0])\n",
    "X_train, y_train_label = (\n",
    "    all_pictures_plus_labels[:size_training_set, :-1],\n",
    "    all_pictures_plus_labels[:size_training_set, -1:]\n",
    ")\n",
    "y_train = transform_label_encoding_to_one_hot(y_train_label, num_classes)\n",
    "#print(y_train_label[:10])\n",
    "#print(y_train[:10])\n",
    "\n",
    "X_val, y_val_label = (\n",
    "    all_pictures_plus_labels[size_training_set:, :-1],\n",
    "    all_pictures_plus_labels[size_training_set:, -1:]\n",
    ")\n",
    "y_val = transform_label_encoding_to_one_hot(y_val_label, num_classes)\n",
    "\n",
    "#X_train, y_train = gen.generate_batch_samples(batch_size=train_size)\n",
    "#X_val, y_val = gen.generate_batch_samples(batch_size=validation_size)\n",
    "#X_test, y_test = gen.generate_batch_samples(batch_size=test_size)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "#N = X_train[0].shape[0]\n",
    "number_of_input_pixels = X_train.shape[1] \n",
    "\n",
    "print(\"total number of pixels {:f}\".format(number_of_input_pixels))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Default parameters. \n",
    "num_iterations = 75\n",
    "val_iteration = 50\n",
    "batch_size = 100\n",
    "weight_decay = 0.03\n",
    "num_hidden_units = [75, 20]\n",
    "learning_rate_li = [0.005, 0.003, 0.0015]\n",
    "\n",
    "params_lin_layer_li = [\n",
    "    {\n",
    "        \"weight_factor\": 1/np.sqrt(number_of_input_pixels), \n",
    "        \"bias_value\": 0.001,\n",
    "        \"learning_rate\": 0.05\n",
    "    },\n",
    "    {\n",
    "        \"weight_factor\": 2/np.sqrt(num_hidden_units[0]),\n",
    "        \"bias_value\":0.001,\n",
    "        \"learning_rate\": 0.03\n",
    "    },\n",
    "    {\n",
    "        \"weight_factor\": 2/np.sqrt(num_hidden_units[1]),\n",
    "        \"bias_value\": 0.001,\n",
    "        \"learning_rate\": 0.02\n",
    "    }\n",
    "]\n",
    "\n",
    "print(params_lin_layer_li)\n",
    "\n",
    "network = Network(batch_size, weight_decay, train_mode=True)\n",
    "network.add_layer(\n",
    "    'linear', dim_out=num_hidden_units[0], \n",
    "    input_dim=number_of_input_pixels, params=params_lin_layer_li[0]\n",
    ")\n",
    "network.add_layer('relu')\n",
    "network.add_layer('linear', dim_out=num_hidden_units[1], \n",
    "                  params=params_lin_layer_li[1])\n",
    "network.add_layer('relu')\n",
    "network.add_layer('linear', dim_out=num_classes, \n",
    "                  params=params_lin_layer_li[2])\n",
    "network.add_loss(SoftmaxCrossEntropyLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(network, X_train, y_train, batch_size)\n",
    "\n",
    "decay_rate = 0.99\n",
    "print(\"starting validation accuracy {}\".format(\n",
    "    get_accuracy(network, X_val, np.argmax(y_val, 1))\n",
    "))\n",
    "validation_accuracy_li = []\n",
    "train_accuracy_li = []\n",
    "for i in range(20):\n",
    "    if i%10==0 and i != 0:\n",
    "        decay_rate = 1\n",
    "    else:\n",
    "        decay_rate = 1\n",
    "    sgd.train(num_iterations, decay_rate, False)\n",
    "    train_accuracy = get_accuracy(network, X_train, np.argmax(y_train, 1))\n",
    "    #print(\"the training accuracy is {}\".format(train_accuracy))\n",
    "    train_accuracy_li.append(train_accuracy)\n",
    "    val_accuracy = get_accuracy(network, X_val, np.argmax(y_val, 1))\n",
    "    #print(\"the validation accuracy is {}\".format(val_accuracy))\n",
    "    validation_accuracy_li.append(val_accuracy)\n",
    "\n",
    "plt.plot(train_accuracy_li)\n",
    "plt.plot(validation_accuracy_li)\n",
    "plt.show()\n",
    "\n",
    "accuracy = get_accuracy(network, X_val, np.argmax(y_val, 1))\n",
    "print(\"the final accuracy on the validation set is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_random_network():\n",
    "    network = Network(batch_size, weight_decay, train_mode=True)\n",
    "    network.add_layer(\n",
    "        'linear', dim_out=num_hidden_units[0],\n",
    "        input_dim=number_of_input_pixels, params=params_lin_layer_li[0]\n",
    "    )\n",
    "    network.add_layer('relu')\n",
    "    network.add_layer('linear', dim_out=num_hidden_units[1], \n",
    "                      params=params_lin_layer_li[1])\n",
    "    network.add_layer('relu')\n",
    "    network.add_layer('linear', dim_out=num_classes, \n",
    "                      params=params_lin_layer_li[2])\n",
    "    network.add_loss(SoftmaxCrossEntropyLoss())\n",
    "    \n",
    "    return network\n",
    "\n",
    "def get_trained_network():\n",
    "    network = get_random_network()\n",
    "    sgd = SGD(network, X_train, y_train, batch_size)\n",
    "    for i in range(60):\n",
    "        sgd.train(num_iterations, learning_rate, False)\n",
    "        val_accuracy = get_accuracy(network, X_val, np.argmax(y_val, 1))\n",
    "        print(\"the validation accuracy is {}\".format(val_accuracy))\n",
    "        if val_accuracy > 0.50:\n",
    "            break\n",
    "            \n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import functions to calculuate information theory measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from information_theory.info_theory import calculate_mutual_information\n",
    "from information_theory.info_theory import points_to_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_to_number(vec):\n",
    "    return np.argmax(vec, axis=1)\n",
    "\n",
    "def create_distributions(X, out, batch_size, max_amount_variables):\n",
    "    \"\"\"compute the average mutual information between input and ouput\n",
    "    \n",
    "    note: the amount of input pixels used is variable  \n",
    "    \"\"\"\n",
    "    \n",
    "    num_pictures = X.shape[0]\n",
    "    unique_out, counts_out = np.unique(out, return_counts=True)\n",
    "    out_dist = counts_out/np.sum(counts_out)\n",
    "    mutual_info_dict = {}\n",
    "    \n",
    "    for i in range(1, max_amount_variables):\n",
    "        pixel_arr = np.zeros((num_pictures, i))\n",
    "        pixel_plus_outcome_arr = np.zeros((num_pictures, i+1))\n",
    "        pixel_plus_outcome_arr[:, -1:] = np.transpose(np.array([out]))\n",
    "        mutual_info_li = []\n",
    "        for batch_number in range(batch_size):\n",
    "            position_points = np.random.randint(grid_len*grid_width, size=i)  \n",
    "            pixel_arr = X[:, position_points]\n",
    "            pixel_plus_outcome_arr[:, :-1] = pixel_arr\n",
    "            \n",
    "            pixel_dist = points_to_dist(pixel_arr)\n",
    "            mutual_dist = points_to_dist(pixel_plus_outcome_arr)\n",
    "            \n",
    "            mutual_info = calculate_mutual_information(pixel_dist, out_dist, mutual_dist)\n",
    "            mutual_info_li.append(mutual_info)\n",
    "            \n",
    "        mutual_info_dict[i] = mutual_info_li\n",
    "        \n",
    "    return mutual_info_dict\n",
    "\n",
    "def get_output_network(network, X, shape_output, number_of_batches):\n",
    "    out_one_hot = np.zeros((shape_output))\n",
    "    batch_size = network.batch_size\n",
    "    for i in range(number_of_batches):\n",
    "        out_one_hot[i*batch_size:(i+1)*batch_size] = (\n",
    "            network.forward(X[i*batch_size:(i+1)*batch_size]) \n",
    "        )\n",
    "    return one_hot_to_number(out_one_hot)\n",
    "    \n",
    "def get_mutual_info_network(get_network, number_of_batches=50, max_amount_variables=10):\n",
    "    gen = generator.GenerateData(grid_len, grid_width, add_noise=False, params=None)\n",
    "    X, y = gen.generate_batch_samples(batch_size=number_of_batches*batch_size)\n",
    "    \n",
    "    batch_size_sampling_distributions = 100\n",
    "    plot_data = []\n",
    "    for i in range(10):\n",
    "        out = get_output_network(get_network(), X, y.shape, number_of_batches)\n",
    "        mutual_info_dict = create_distributions(X, out, batch_size_sampling_distributions, max_amount_variables)\n",
    "        plot_data.append([np.mean(v) for k, v in mutual_info_dict.items()])\n",
    "        #for k, v in mutual_info_dict.items():\n",
    "        #    print(\"number of variables {}, mean mutual info {}\".format(k, np.mean(v)))\n",
    "\n",
    "    return plot_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"get a random network\")\n",
    "plot_data_random_network = get_mutual_info_network(get_random_network)\n",
    "print(\"get a trained network\")\n",
    "plot_data_trained_network = get_mutual_info_network(get_trained_network)\n",
    "plot_data_actual_values = []\n",
    "for i in range(10):\n",
    "    out = one_hot_to_number(out_one_hot)\n",
    "    mutual_info_dict = create_distributions(X, out, batch_size=30, max_amount_variables=10)\n",
    "    plot_data_actual_values.append([np.mean(v) for k, v in mutual_info_dict.items()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " to calculate mutual information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_freq_table(df, output_network):    \n",
    "    square = output_network==1\n",
    "    circle = output_network==2\n",
    "    rectangle = output_network==3\n",
    "\n",
    "    df[\"square\"] = pd.Series(square.astype('int'))\n",
    "    df[\"circle\"] = pd.Series(circle.astype('int'))\n",
    "    df[\"rectangle\"] = pd.Series(rectangle.astype('int'))\n",
    "\n",
    "    print(df)\n",
    "    grouped = df.groupby(list(df.columns[:-3]), as_index=True)\n",
    "    df2 = grouped.aggregate(np.sum)\n",
    "    print(df2)\n",
    "\n",
    "    print(df2.iloc[:, -3:])\n",
    "    print(df2.iloc[:, -3:].apply(np.sum, 1))\n",
    "    df2['x_freq'] = df2.iloc[:, -3:].apply(np.sum, 1)\n",
    "\n",
    "    return df2\n",
    "\n",
    "def one_hot_to_number(vec):\n",
    "    return np.argmax(vec, axis=1)\n",
    "\n",
    "def get_output_network(network, X, number_of_classes, number_of_batches):\n",
    "    \"\"\"produce the ouput of the network for all pictures in X\"\"\"\n",
    "    out_distribution = np.zeros(X.shape[0], number_of_classes)\n",
    "    batch_size = network.batch_size\n",
    "    for i in range(number_of_batches):\n",
    "        out_distribution[i*batch_size:(i+1)*batch_size] = (\n",
    "            network.forward(X[i*batch_size:(i+1)*batch_size]) \n",
    "        )\n",
    "    network.update_batch_size(1)\n",
    "    for i in range(number_of_batches*network.batch_size, X.shape[0])\n",
    "        out_distribution[i] = network.forward(X[i]) \n",
    "        \n",
    "    network.update_batch_size(batch_size)\n",
    "    \n",
    "    #make sure batch_size is set back correctly\n",
    "    network.forward(X[:batch_size])\n",
    "    \n",
    "    return one_hot_to_number(out_distribution)\n",
    "        \n",
    "def calculate_mutual_info_distribution():\n",
    "    \"\"\"return a li with all mutual info values\"\"\"\n",
    "    return []\n",
    "    \n",
    "def calculate_mutual_information(freq_df):\n",
    "    \"\"\"calculate the mutual information given dataframe \n",
    "       holding the frequencies\n",
    "    \n",
    "    returns: the mutual information\n",
    "    \"\"\"\n",
    "    #take the sum of every category column divide by total\n",
    "    total_number_of_outcomes = freq_df[\"x_freq\"].sum()\n",
    "    if all_pictures.shape[0] != total_number_of_outcomes:\n",
    "        raise ValueError(\"calculation fo freq_df has gone wrong\")\n",
    "\n",
    "    output_distribution = {}\n",
    "    for category in freq_df.colums:\n",
    "        if category != \"x_freq\":\n",
    "            output_distribution[category] = (\n",
    "                freq_df[category].sum()/total_number_of_outcomes\n",
    "            )\n",
    "            \n",
    "    dist_x = freq_df.loc[:, [\"x_freq\"]]/freq_df.loc[:, [\"x_freq\"]].sum()\n",
    "    mutual_information = 0\n",
    "    for category in freq_df.colums:\n",
    "        if category != \"x_freq\":\n",
    "            remove_zeros_mask = freq_dif.loc[:, [category]] != 0\n",
    "            prob_x_given_y = (\n",
    "                freq_df[remove_zeros_mask].loc[:, category] /\n",
    "                freq_df.loc[:, category].sum()\n",
    "            )\n",
    "            mutual_information = mutual_information + (\n",
    "                prob_x_given_y * \n",
    "                output_distribution[category] *\n",
    "                prob_x_given_y.divide(dist_x[remove_zeros_mask]).map(np.log2)\n",
    "            )\n",
    "            \n",
    "    return mutual_information\n",
    "\n",
    "def batch_sample_mutual_info(all_pictures, number_of_networks, \n",
    "                             max_selected_input_pixels, number_of_positions):\n",
    "    \"\"\"calculate average mutual information between n \n",
    "       input pixels and output by sampling for different \n",
    "       networks, position of pixels and amount of input pixels\n",
    "    \n",
    "    returns: a list of dicts, every entry in the list resembles one\n",
    "        network. The keys from the dict are the amount of pixels sampled\n",
    "        from and the values the calculated mutual information scores \n",
    "        for different positions\n",
    "    \"\"\"\n",
    "    network_mutual_info_li = []\n",
    "    for i in range(number_of_networks):\n",
    "        network = get_trained_network()\n",
    "        output_network = get_output_network(network, all_pictures)\n",
    "        if output.shape[0] != X.shape[0]:\n",
    "            raise ValueErrror(\"something went wrong while calculating output\")\n",
    "            \n",
    "        mutual_information_for_n_pixels = {}\n",
    "        for number_of_pixels in range(max_selected_input_pixels):\n",
    "            mutual_information_li = []\n",
    "            for sample_pos in range(number_of_positions):\n",
    "                #select position of pixels at random\n",
    "                position_pixels = np.random.randint(\n",
    "                    all_pictures.shape[1], number_of_pixels\n",
    "                )\n",
    "                sel_pixels_all_pictures = all_pictures[:, position_pixels]\n",
    "                freq_df = create_freq_table(\n",
    "                    pd.DataFrame(sel_pixels_all_pictures),\n",
    "                    output_network\n",
    "                )\n",
    "                mutual_information_li.append(calculate_mutual_information(freq_df))\n",
    "                \n",
    "            mutual_information_for_n_pixels[number_of_pixels] = mutual_information_li\n",
    "            \n",
    "        network_mutual_info_li.append(mutual_information_for_n_pixels)\n",
    "            \n",
    "    return network_mutual_info_li\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second smarter way to calculate mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_prob(network, pos_pixels, value_pixels, y, y_hat):\n",
    "    #from all pictures select those where y, y_hat match\n",
    "    sel_pictures = [picture for picture in pictures[y] \n",
    "                    if network.calculate_forward(picture) == y_hat]\n",
    "\n",
    "    #from those pictures calculate what percentage has \n",
    "    #the values of the pixels at the right positions\n",
    "    if sel_pictures == []:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(\n",
    "            [1 for picture in sel_pictures if picture[pos_pixels] == value_pixels]\n",
    "        )/len(sel_pictures)\n",
    "\n",
    "def calculate_probability(network, pos_pixels, value_pixels):\n",
    "    #calculate P(x|y_hat) for every value of y_hat \n",
    "    #x represents the value of certain pixels and y_hat the output of the ANN\n",
    "    \n",
    "    prob_li = []\n",
    "    for y_hat in range(num_classes):\n",
    "        #calculate p(y|y_hat) and p(y_hat) upfront \n",
    "        #prob = sum_y (p(x|y, y_hat) * p(y|y_hat) * p(y_hat))\n",
    "        prob = 0\n",
    "        for y in range(num_classes):\n",
    "            prob = prob + (\n",
    "                get_prob(network, pos_pixels, value_pixels, y, y_hat) *\n",
    "                prob_y_given_y_hat[(y, y_hat)] *\n",
    "                prob_y_hat[y_hat]\n",
    "            )\n",
    "        prob_li.append(prob)\n",
    "        \n",
    "    return prob_li\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.legend(loc=2,prop={'size':6})\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "\n",
    "plot_data = np.arange(0, 9, 1)\n",
    "plot_trained_network_mean = np.mean(np.array(plot_data_trained_network), axis=0)\n",
    "plot_trained_network_std = np.std(np.array(plot_data_trained_network), axis=0)\n",
    "lower_bound_trained_network = plot_trained_network_mean-plot_trained_network_std \n",
    "upper_bound_trained_network = plot_trained_network_mean+plot_trained_network_std\n",
    "\n",
    "plot_random_network_mean = np.mean(np.array(plot_data_random_network), axis=0)\n",
    "plot_random_network_std = np.std(np.array(plot_data_random_network), axis=0)\n",
    "lower_bound_random_network = plot_random_network_mean-plot_random_network_std\n",
    "upper_bound_random_network = plot_random_network_mean+plot_random_network_std \n",
    "\n",
    "plot_actual_values_mean = np.mean(np.array(plot_data_actual_values), axis=0)\n",
    "plot_actual_values_std = np.std(np.array(plot_data_actual_values), axis=0)\n",
    "\n",
    "plt.plot(plot_trained_network_mean, label=\"mean trained network\")\n",
    "plt.fill_between(plot_data, lower_bound_trained_network, \n",
    "                 upper_bound_trained_network, alpha=0.3)\n",
    "\n",
    "plt.plot(plot_random_network_mean, label=\"mean random network\")\n",
    "plt.fill_between(plot_data, lower_bound_random_network, \n",
    "                 upper_bound_random_network, alpha=0.3)\n",
    "\n",
    "plt.plot(plot_actual_values_mean, label=\"mean actual outputs\")\n",
    "#plt.fill_between(plot_data, \n",
    "#                 plot_actual_values_mean-plot_actual_values_std,\n",
    "#                 plot_random_network_mean+plot_actual_values_std,\n",
    "#                 alpha=0.3)\n",
    "\n",
    "plt.title(\"Mutual information between randomly selected pixels and network output\")\n",
    "plt.xlabel(\"number of variables\")\n",
    "plt.ylabel(\"mean mutual information between pixels and output network\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.DataFrame(np.array([[1,2,1],[1,2,2],[1,2,1], [1,1,1], [1,2,2], [2,1,2]]))\n",
    "print(\"a equals\")\n",
    "print(list(a.columns))\n",
    "\n",
    "b = a.groupby([0,1,2])\n",
    "c = pd.DataFrame(b.size()).values.flatten()\n",
    "print(\"c equals\")\n",
    "print(c)\n",
    "\n",
    "d = c/c.sum()\n",
    "print(d)\n",
    "print(d.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "out_come_ANN = {\n",
    "    \"square\": np.array([[0,1,1],[0,0,1],[0,1,1]],\n",
    "                       [[0,0,0],[0,0,1],[0,1,1]], \n",
    "                       [[1,0,0],[1,0,1],[1,1,1]])\n",
    "\n",
    "data_len = len([out_come_ANN[cat].shape[0] for cat in out_come_ANN.keys()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the algorithm to generate the appropriate distributions is outlined on a simple test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "picture_selected_pixels = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(picture_selected_pixels)\n",
    "\n",
    "classification_deep_network = np.array(\n",
    "    [1, 3, 2, 3, 2, 3, 3]\n",
    ")\n",
    "\n",
    "def create_freq_tables(df, output_network):    \n",
    "    square = output_network==1\n",
    "    circle = output_network==2\n",
    "    rectangle = output_network==3\n",
    "\n",
    "    df[\"square\"] = pd.Series(square.astype('int'))\n",
    "    df[\"circle\"] = pd.Series(circle.astype('int'))\n",
    "    df[\"rectangle\"] = pd.Series(rectangle.astype('int'))\n",
    "\n",
    "    print(df)\n",
    "    grouped = df.groupby(list(df.columns[:-3]), as_index=True)\n",
    "    df2 = grouped.aggregate(np.sum)\n",
    "    print(df2)\n",
    "\n",
    "    print(df2.iloc[:, -3:])\n",
    "    print(df2.iloc[:, -3:].apply(np.sum, 1))\n",
    "    df2['x_freq'] = df2.iloc[:, -3:].apply(np.sum, 1)\n",
    "\n",
    "    return df2\n",
    "    \n",
    "print(create_freq_tables(df, classification_deep_network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "\n",
    "a[:, np.array([0,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
